# (PART) Example {-}

```{r echo=FALSE}
library(checkdown)
```

# Example of Time Series Data
```{r}
library(astsa)
```

## Characteristics of Time Series 
### Practical data
判斷以下範例為何不是 (weakly) stationary

<mark>Example 1.1 (p.2)</mark>
```{r}
plot(jj, type="o", ylab="Quarterly Earnings per Share")
```

```{r echo=FALSE}
check_question(c("trend", "seasonality", "heteroskedasticity"), 
               options = c("trend", "seasonality", "heteroskedasticity"), 
               type = "checkbox", wrong = "**NOPE!**")
```
<hr>

<mark>Example 1.2 (p.3)</mark>
```{r}
plot(globtemp, type="o", ylab="Global Temperature Deviations")
```

```{r echo=FALSE}
check_question("trend", 
               options = c("trend", "seasonality", "heteroskedasticity"), 
               type = "checkbox", wrong = "**NOPE!**")
```
<hr>

<mark>Example 1.3 (p.3) Voice record</mark>
```{r}
plot(speech)
```

```{r echo=FALSE}
check_question("seasonality", 
               options = c("trend", "seasonality", "heteroskedasticity"), 
               type = "checkbox", wrong = "**NOPE!**")
```
<hr>

<mark>Example 1.5 (p.5)</mark> 
```{r}
par(mfrow = c(2,1)) # set up the graphics
plot(soi, ylab="", xlab="", main="Southern Oscillation Index\nindex of ocean ccurrent 洋流")
plot(rec, ylab="", xlab="", main="Recruitment\nindex of fish amount")
```

```{r echo=FALSE}
check_question("seasonality", 
               options = c("trend", "seasonality", "heteroskedasticity"), 
               type = "checkbox", wrong = "**NOPE!**")
```
<hr>

<mark>Example 1.6 (p.5) MRI data</mark>
```{r}
par(mfrow=c(2,1))
ts.plot(fmri1[,2:5], col=1:4, ylab="BOLD", main="Cortex")
ts.plot(fmri1[,6:9], col=1:4, ylab="BOLD", main="Thalamus & Cerebellum")
mtext("Time (1 pt = 2 sec)",side = 1,line = 2)
```

```{r echo=FALSE}
check_question("seasonality", 
               options = c("trend", "seasonality", "heteroskedasticity"), 
               type = "checkbox", wrong = "**NOPE!**")
```
<hr>

<mark>Example 1.7 (p.6)</mark>
```{r}
par(mfrow=c(2,1))
plot(EQ5, main="Earthquake")
plot(EXP6, main="Explosion")
```

```{r echo=FALSE}
check_question("heteroskedasticity", 
               options = c("trend", "seasonality", "heteroskedasticity"), 
               type = "checkbox", wrong = "**NOPE!**")
```
<hr>

### White Noise (WN)
<mark>Example 1.9 (p.10)</mark>
```{r}
set.seed(0921)
w = rnorm(500,0,1) # 500 N(0,1) variates
v = filter(w, sides=2, filter=rep(1/3,3)) # moving average
par(mfrow=c(2,1))
plot.ts(w, main="white noise")
plot.ts(v, ylim=c(-3,3), main="moving average")
```
$$W_t\sim N(0,1)$$

* sides=2, center at lag 0

$$V_t=\frac1 3(W_{t-1}+W_t+W_{t+1})$$

* sides=1, for past value only

$$V_t=\frac1 3(W_t+W_{t-1}+W_{t-2})$$
<hr>

<mark>Example 1.10 (p.11)</mark>  
```{r}
set.seed(0921)
w = rnorm(550,0,1) # 50 extra to avoid startup problems
x = filter(w, filter=c(1,-.9), method="recursive")[-(1:50)] # remove first 50
plot.ts(x, main="autoregression")
```
$$W_t\sim N(0,1)$$
$X_t=W_t-0.9W_{t-1}$ (default sides=1)

$X_t$ and $X_{t+1}$ are **negatively** correlated, means when  
<div>
  * $X_t\uparrow\:\Rightarrow X_{t+1}\downarrow$  
  * $X_t\downarrow\:\Rightarrow X_{t+1}\uparrow$  
</div>
<hr>

### Random Walk
<mark>Example 1.11 (p.11)</mark>
```{r}
set.seed(154) # so you can reproduce the results
w = rnorm(200,0,1); x = cumsum(w) # two commands in one line
wd = w +.2; xd = cumsum(wd)
plot.ts(xd, ylim=c(-5,55), main="random walk")
lines(x, col=2)
lines(.2*1:200,lty="dashed")
legend('topleft', col=c(1,2,1), lty=c(1,1,2), 
       legend = c(expression(X[d]),expression(X[t]),expression(E(X[t]))))
```
$X_t$ is not stationary, $\because Var(X_t)$ depends on t.

$X_d$ is not stationary, $\because E(X_d)$ depends on t.

* $W_t\sim N(0,1)$
* $X_t=W_1+W_2+...+W_t$
  * $Var(X_t)=Var(W_1)+Var(W_2)+...+Var(W_t)=1\cdot t=t$
* $W_d=W_t+0.2$
* $X_d=\sum W_d=\sum W_t+0.2\cdot t$
  * $E(X_d)=0.2\cdot t$

> | $E(aX+b)=aE(X)+b$
  | $Var(aX+b)=a^2Var(X)$

<hr>

### Noise influence periodic
<mark>Example 1.12 (p.12)</mark>
```{r}
cs = 2*cos(2*pi*1:500/50 + .6*pi); w = rnorm(500,0,1)
par(mfrow=c(3,1), mar=c(3,2,2,1), cex.main=1.5)         # help(par) for info
plot.ts(cs, main=expression(x[t]==2*cos(2*pi*t/50+.6*pi)))
plot.ts(cs+w, main=expression(x[t]==2*cos(2*pi*t/50+.6*pi) + N(0,1)))
plot.ts(cs+5*w, main=expression(x[t]==2*cos(2*pi*t/50+.6*pi) + N(0,25)))
```
The periodic behavior became less obvious when the noise is large.

<hr>

### How lag influence plot
<mark>Example 1.24 (p.24)</mark>
$$y_t=Ax_{t-l}+w_t$$

* $l>0$, $x_t$ **lead** $y_t$
  * $y_t=x_{t-1}+w_t$
  * $x$ lead $y$
* $l<0$, $x_t$ **lag** $y_t$
  * $y_t=x_{t+1}+w_t$
  * $x$ lag $y$

Assume $w_t$ is uncorrelated with $x_t$, the cross-covariance function is $$\gamma_{yx}(h)=cov(y_{t+h},x_t)=cov(Ax_{t+h-l}+w_{t+h},x_t)\\=cov(Ax_{t+h-l},x_t)=A\gamma_x(h-l)$$

When $h=l$, $\gamma_{yx}(h)=\gamma_x(0)$

```{r}
x = rnorm(100)
y = lag(x, -5) + rnorm(100)
par(mfrow=c(1,1))
ccf <- ccf(y, x, type='covariance', 
           ylab='CCovF', xlab='lag h', main=expression(paste('l=5 with', hat(gamma)[yx](h))))
```

* $X\sim N(0,1)$
* $y_t=X_{t-5}+W_t, W_t\sim N(0,1)$
* $\require{enclose} \enclose{horizontalstrike}{Corr(y_t, X_{t+h})=\big\{\substack{1,\:h=5\\0,\:o.w.}}$
* $l=5\to\gamma_{yx}(h)=\gamma_x(h-5)$
  * $lag\:h=5 \to\gamma_{yx}(5)=\gamma_x(0)=variance(X)=1$ 
  * $y_{t+h}=Ax_{t+h-5}+w_{t+h}$
    * $i=|h-l|=|h-5|$
    * $h>5\to\gamma_{yx}(h)=cov(Ax_{t+i},x_t)\to x\:leads$
    * $h<5\to\gamma_{yx}(h)=cov(Ax_{t-i},x_t)\to y\:leads$
    
<hr>

<mark>Example 1.25 (p.28)</mark>
```{r}
par(mfrow=c(1,2))
r = round(acf(soi, 6, plot=FALSE)$acf[-1], 3)
plot(lag(soi,-1), soi, ylab=expression(X[t]), xlab=expression(X[t-1]))
arrows(-1,-1,1,1, col=2, angle = 15, lwd=5)
legend('topleft', legend=r[1])
plot(lag(soi,-6), soi, ylab=expression(X[t]), xlab=expression(X[t-6]))
arrows(-1,1,1,-1, col=2, angle = 15, lwd=5)
legend('topleft', legend=r[6])
```

___

## Explore Data 
### Estimate a linear trend
<mark>Example 2.1 (p.45)</mark>
$$x_t=a+b\cdot t+W_t$$
```{r}
summary(fit <- lm(chicken~time(chicken)))
plot(chicken, type="o",ylab="cents per pound")
abline(fit, lwd=2, col=2) # add regression line to the plot
legend('topleft', expression(a+bt), lwd=2, col=2)
```

<hr>

### Detrend and Differencing
<mark>Example 2.4 and 2.5 (p.54, 58)</mark>
$$x_t=a+b\cdot t+W_t$$

* detrended: $X_t-a-b\cdot t$
* first difference: $\nabla X_t=X_t-X_{t-1}$
```{r}
fit = lm(chicken~time(chicken), na.action=NULL) # regress chicken on time
par(mfrow=c(2,1), mar=c(3,4,2,1))
plot(resid(fit), type="o", main="detrended")
plot(diff(chicken), type="o", main="first difference")

par(mfrow=c(3,1), mar=c(4,3,3,1)) # plot ACFs
acf(chicken, 48, main="chicken")
acf(resid(fit), 48, main="detrended")
acf(diff(chicken), 48, main="first difference")
```

* first difference does better than regression
* ACF of stationary process decays fast to zero

<hr>

<mark>Example 2.6 (p.58)</mark>
```{r}
par(mfrow=c(2,1), mar=c(4,4,3,1))
plot(diff(globtemp), type="o")
mean(diff(globtemp)) # drift estimate = .008
acf(diff(gtemp), 48)
```

* look stationary after first-order differencing

<hr>

### Smoothing
<mark>Example 2.13 (p.67) lowess</mark>

* `lowess(x, y = NULL, f = 2/3)`
  * `f`: the smoother span (band width)
```{r}
plot(soi)
lines(lowess(soi, f=.05), lwd=2, col=4) # El Nino cycle
lines(lowess(soi), lty=2, lwd=2, col=2) # trend (with default span)
legend('topright', c('seasonality','trend'), col=c(4,2), lty=c(1,2), lwd=2)
```

* seasonality: small band width
* trend: large band width

<hr>

<mark>Example 2.14 (p.68) smooth.spline</mark>

* `smooth.spline(x, y, spar = NULL)`
  * `x`: predictor
  * `y`: response
  * `spar`: smoothing parameter, typically (but not necessarily) in (0,1]
  
```{r}
plot(soi)
lines(smooth.spline(time(soi), soi, spar=.5), lwd=2, col=4)
lines(smooth.spline(time(soi), soi, spar= 1), lty=2, lwd=2, col=2)
legend('topright', c('seasonality','trend'), col=c(4,2), lty=c(1,2), lwd=2)
```

<hr>

## Time Series model
<mark>Example 3.2 (p.78) AR(1)</mark>

* `arima.sim`: Simulate from an ARIMA model.
  * `order`: `c(p, d, q)` $\to$ AR order, degree of differencing, MA order. 
  * ARIMA(0,0,0) is white noise.
* If AR(2) $\to$ `order=c(2,0,0), ar=c(.5, .1)`

```{r}
par(mfrow=c(2,1), mar=c(3,4,2,1))
plot(arima.sim(list(order=c(1,0,0), ar=.9), n=100), ylab="x",
     main=(expression(AR(1)~~~phi==+.9)))
plot(arima.sim(list(order=c(1,0,0), ar=-.9), n=100), ylab="x",
     main=(expression(AR(1)~~~phi==-.9)))
```

|top|bottom|
|:-|:-|
|$\rho(h)=\color{red}{(0.9)}^h, h\geq0$|$\rho(h)=\color{red}{(-0.9)}^h, h\geq0$|
|positively correlated|negatively correlated|
|look like trend|fluctuate rapidly|

___

<mark>Example 3.5 (p.82) MA</mark>
MA(1) model $x_t=w_t+\theta w_{t-1}$
$$
E(x_t)=0\\
\gamma(h)=\Bigg\{\begin{array}{ll}
(1+\theta^2)\sigma^2_w,&h=0\\
\theta\sigma^2_w,&h=1\\
0,&h>1
\end{array}\\
\rho(h)=\Big\{\begin{array}{ll}
\frac{\theta}{1+\theta^2},&h=1\\
0,&h>1
\end{array}
$$

```{r}
par(mfrow = c(2,1), mar=c(3,4,2,1))
plot(arima.sim(list(order=c(0,0,1), ma=.9), n=100), ylab="x",
     main=(expression(MA(1)~~~theta==+.5)))
plot(arima.sim(list(order=c(0,0,1), ma=-.9), n=100), ylab="x",
     main=(expression(MA(1)~~~theta==-.5)))

```

|top|bottom|
|:-|:-|
|$\theta=0.5\Rightarrow x_t, x_{t-1}$ are positively correlated|$\theta=-0.5\Rightarrow x_t, x_{t-1}$ are negatively correlated|
|$\rho(1)=\frac{0.5}{1+0.5^2}=0.4\Rightarrow$ weak dependence| |

* The series for which $\theta=0.5$ is smoother than the series for $\theta=-0.5$.

___

<mark>Example 3.7 (p.84) parameter redundency</mark>
If we were unaware of parameter redundancy, we might claim the
data are correlated when in fact they are not

$$
X_t\sim N(5,1),\text{ fit ARIMA(1,1) to X}\\
X_t=\phi X_{t-1}+W_t+\theta W_{t-1}
$$

```{r}
set.seed(8675309)
x = rnorm(150, mean=5) # generate iid N(5,1)s
out <- arima(x, order=c(1,0,1)); out # estimation
```

* $\phi=$ `r round(out$coef[1],2)`, $\theta=$ `r round(out$coef[2],2)`
* $(1+0.96B)X_t=(1+0.95B)W_t$

___

<mark>Example 3.8 (p.86) ARMA to MA</mark>
```{r}
ARMAtoMA(ar = .9, ma = .5, 10) # for a list
plot(ARMAtoMA(ar = .9, ma = .5, 10), 
     ylab = expression(psi [j]))
ARMAtoMA(ar = -.5, ma = -.9, 10) # first 10 pi-weights
```

___

<mark>Example 3.11 (p.92) AR(2)</mark>
```{r}
z = c(1,-1.5,.75) # coefficients of the polynomial
(a = polyroot(z)[1]) # = 1+0.57735i,print one root = 1 + i/sqrt(3)
```

* `polyroot`: 解根

$$f(x)=1-1.5x+0.75x^2$$
```{r}
arg = Arg(a)/(2*pi) # arg in cycles/pt
1/arg # =12,the period
```

$$
z=x+iy\\
r=\sqrt{x^2+y^2}\\
\phi=Arg(z)\\
x=r\cdot cos(\phi),\;y=r\cdot sin(\phi)
$$
```{r}
set.seed(90210)
ar2 = arima.sim(list(order=c(2,0,0), ar=c(1.5,-.75)), n = 144)  # simulated data
plot(1:144/12,ar2,type="l",xlab="Time (one unit = 12 obs)")
abline(v=0:12, lty="dotted",lwd=2)
```

$$X_t=1.5X_{t-1}-0.75X_{t-2}+W_t$$

* has seasonality with period $d=12$
* AR(2) process is not stationary
* MA(q) process is always stationary

```{r}
ACF = ARMAacf(ar=c(1.5,-.75), ma=0, 50) # theoretical
par(mfrow = c(2,1), mar=c(3,4,3,1))
plot(ACF, type="h", xlab="lag", main="theoretical ACF")
abline(h=0)
acf(ar2, lag.max = 50, main="sample ACF")
```

* `ARMAacf`: theoretical ACF (when model is given)
* `acf`: sample ACF (when data is given)
* ACF has periodic pattern

___

<mark>Example 3.16 (p.98) PACF of AR(p)</mark>
```{r}
ar2.acf = ARMAacf(ar=c(1.5,-.75), ma=0, 24)[-1]
ar2.pacf = ARMAacf(ar=c(1.5,-.75), ma=0, 24, pacf=TRUE)
par(mfrow=c(1,2), mar=c(4,4,1,1))
plot(ar2.acf, type="h", xlab="lag"); abline(h=0)
plot(ar2.pacf, type="h", xlab="lag")
abline(h=0); axis(1, at=c(1:5))
```

* PACF: cut off after lag 2

___

<mark>Example 3.29 (p.115) Simulate 50 obs. from MA(1) with $\theta_1=0.9$</mark>
```{r}
set.seed(2)
ma1 = arima.sim(list(order = c(0,0,1), ma = 0.9), n = 50)
acf(ma1, plot=FALSE)[1] # = .507 (lag 1 sample ACF)
```

* True ACF: $\rho(1)=\frac{\theta_1}{1+\theta_1^2}=$ `r 0.9/(1+0.9^2)`

___

### Behavior of the ACF and PACF for ARMA models

| |AR(p)|MA(q)|ARMA(p, q)|
|:-|:-|:-|:-|
|ACF|Tails off|Cuts off after lag q|Tails off|
|PACF|Cuts off after lag p|Tails off|Tails off|

## Fit model with different method
在對資料進行建模前，可以用ACF, PACF初步分析，判斷p, q需要選多少

### Preliminary Analysis
<mark>Example 3.18 (p.99) Preliminary Analysis of the Recruitment Series</mark>
```{r}
value <- acf2(rec, 48) # will produce values and a graphic
```

* ACF decay very fast $\to$ stationary
* PACF cut-off after lag 2 (its PACF $\approx$ 0 after lag 2) $\to$ AR(2) is suitable for the data

```{r echo=FALSE}
knitr::kable(value[,1:16], col.names = c(1:16))
knitr::kable(value[,17:32], col.names = c(17:32))
knitr::kable(value[,33:48], col.names = c(33:48))
```

___

### Fit AR(2) model to Recruitment Series
<mark>Example 3.25 (p.110) Fit AR(2) to rec data by OLS</mark>
```{r}
regr = ar.ols(rec, order=2, demean=FALSE, intercept=TRUE)
fore = predict(regr, n.ahead=24)
ts.plot(rec, fore$pred, col=1:2, xlim=c(1980,1990), ylab="Recruitment")

# 95% CI
U = fore$pred+1.96*fore$se; L = fore$pred-1.96*fore$se
xx = c(time(U), rev(time(U))); yy = c(L, rev(U))
polygon(xx, yy, border = 8, col = gray(.6, alpha = .2))
lines(fore$pred, type="p", col=2)
```

<mark>Example 3.28 (p.115) Fit AR(2) to rec data by Yule-Walker</mark>
```{r results='hide'}
rec.yw = ar.yw(rec, order=2)
rec.yw$x.mean # = 62.26278 (mean estimate)
rec.yw$ar # = 1.3315874, -.4445447 (coefficient estimates)
sqrt(diag(rec.yw$asy.var.coef)) # = .04222637, .04222637 (standard errors)
rec.yw$var.pred # = 94.79912 (error variance estimate)
```
```{r}
rec.pr = predict(rec.yw, n.ahead=24)
# 95% CI
U=rec.pr$pred+1.96*rec.pr$se
L=rec.pr$pred-1.96*rec.pr$se
minx=min(rec,L);maxx=max(rec,U)
ts.plot(rec, rec.pr$pred, xlim=c(1980,1990),ylim=c(minx,maxx))
lines(rec.pr$pred,col="red",type="o")
lines(U, col="blue", lty="dashed")
lines(L, col="blue", lty="dashed")
```

<mark>Example 3.31 (p.120) Fit AR(2) to rec data by MLE</mark>
```{r results='hide'}
rec.mle = ar.mle(rec, order=2)
rec.mle$x.mean # 62.26
rec.mle$ar # 1.35, -.46
sqrt(diag(rec.mle$asy.var.coef)) # .04, .04
rec.mle$var.pred # 89.34
```

### compare AR(2) model's coef. estimated by OLS, YW, and MLE
|OLS|YW|MLE|
|:-|:-|:-|
|`r round(regr$ar, 3)`|`r round(rec.yw$ar, 3)`|`r round(rec.mle$ar, 3)`|

* Its coefficient estimator is similar

___

## Analysis of GNP Data
<mark>Example 3.39, 3.40 (p.136, 139)</mark>
```{r}
plot(gnp)
val <- acf2(gnp, 50)
```

* ACF looks not stationary

```{r}
gnpgr = diff(log(gnp)) # growth rate
```

* $diff(log(X_t))=logX_t-logX_{t-1}=log\frac{X_t}{X_{t-1}}, \;growth$
  * $diff(log(X_t))>0, \uparrow$
  * $diff(log(X_t))<0, \downarrow$

```{r}
plot(gnpgr, ylab="GNP growth rate", lwd=2)
abline(h=mean(gnpgr), lty=2, col=2, lwd=2)
legend("topright", legend = "average growth rate", lty=2, lwd=2, col=2)
val <- acf2(gnpgr, 24)
```

* ACF cut-off after lag 2 $\Rightarrow$ MA(2)
* PACF cut-off after lag 1 $\Rightarrow$ AR(1)

```{r echo=FALSE}
knitr::kable(val[,1:12], col.names = c(1:12))
knitr::kable(val[,13:24], col.names = c(13:24))
```

### Diagnostics 
```{r results='hide'}
out1 <- sarima(gnpgr, 1, 0, 0) # AR(1)
out2 <- sarima(gnpgr, 0, 0, 2) # MA(2)
```

* In Ljung-Box statistic, both model's p-value>0.05, $\therefore$ AR(1) is suitable for `gnpgr`

```{r}
ARMAtoMA(ar=.35, ma=0, 10) # prints psi-weights
```

### model selection
Both AR(1) and MA(2) are suitable. Then, which one is better?

* Use AIC, AICC or BIC, *the smaller the better*. 

|model|AIC|AICc|BIC|
|:-|:-|:-|:-|
|AR(1)|`r out1$AIC`|`r out1$AICc`|`r out1$BIC`|
|MA(2)|`r out2$AIC`|`r out2$AICc`|`r out2$BIC`|

* Since MA(2) model's AIC, AICC and BIC are smaller than AR(1) model's, MA(2) is better. 

___

## Model Diagnostics
<mark>Example 3.41 (p.140) Diagnostics for the Glacial Varve Series</mark>
```{r results='hide'}
sarima(log(varve), 0, 1, 1, no.constant=TRUE) # ARIMA(0,1,1)
```

* ACF outside the CI, and p-value$\leq$0.05, $\therefore$ ARIMA(0,1,1) is not suitable for `log(varve)`

```{r results='hide'}
sarima(log(varve), 1, 1, 1, no.constant=TRUE) # ARIMA(1,1,1)
```

* all ACF inside the CI, and all p-value above 0.05, $\therefore$ ARIMA(1,1,1) is suitable for `log(varve)`.

___

##  Regression with Autocorrelated Errors
<mark>Example 3.44 (p.144) Mortality, Temperature and Pollution</mark>

* 配適模型查看溫度(temperature)、顆粒物水平(particulate levels)與心血管死亡率(cardiovascular mortality)的關係

$$M_t=\beta_1+\beta_2t+\beta_3T_t+\beta_4T^2_t+\beta_5P_t+x_t$$

```{r}
trend = time(cmort); temp = tempr - mean(tempr); temp2 = temp^2
fit <- lm(cmort~trend + temp + temp2 + part, na.action=NULL)
round(summary(fit)$coef, 3) |> knitr::kable()
```
```{r results='hide'}
acf2(resid(fit), 52) # implies AR2
sarima(cmort, 2,0,0, xreg=cbind(trend,temp,temp2,part))
```

* ACF outside the CI, but the p-value are above 0.05, $\therefore$ AR(2) is not good enough for `cmort`.

___

<mark>Example 3.45 (p.145) Regression with Lagged Variables</mark>

$$R_t=\beta_0+\beta_1S_{t−6}+\beta_2D_{t−6}+\beta_3D_{t−6}S_{t−6}+wt$$

```{r}
dummy = ifelse(soi<0, 0, 1)
fish = ts.intersect(rec, soiL6=lag(soi,-6), dL6=lag(dummy,-6), dframe=TRUE)
fit <- lm(rec ~ soiL6*dL6, data=fish, na.action=NULL)
round(summary(fit)$coef, 3) |> knitr::kable()

attach(fish)
plot(resid(fit))
```
```{r results='hide'}
acf2(resid(fit)) # indicates AR(2)

intract = soiL6*dL6 # interaction term
sarima(rec,2,0,0, xreg = cbind(soiL6, dL6, intract))
```

* ACF outside the CI, and the p-value are below 0.05, $\therefore$ AR(2) is not suitable.

___

## Multiplicative Seasonal ARIMA Models
* 先一階差分再看季節性
~~* 季節性資料優先處理季節性，再看需不需要差分~~

<mark>Example 3.46 (p.146) A Seasonal AR Series</mark>

$$(1 − 0.9\cdot B^{12})x_t = w_t$$

```{r}
set.seed(666)
phi = c(rep(0,11),.9)
sAR = arima.sim(list(order=c(12,0,0), ar=phi), n=37)
sAR = ts(sAR, freq=12)
layout(matrix(c(1,1,2, 1,1,3), nc=2))
par(mar=c(3,3,2,1), mgp=c(1.6,.6,0))
plot(sAR, axes=FALSE, main='seasonal AR(1)', xlab="year", type='c')
Months = c("J","F","M","A","M","J","J","A","S","O","N","D")
points(sAR, pch=Months, cex=1.25, font=4, col=1:4)
axis(1, 1:4); abline(v=1:4, lty=2, col=gray(.7))
axis(2); box()
ACF = ARMAacf(ar=phi, ma=0, 100)
PACF = ARMAacf(ar=phi, ma=0, 100, pacf=TRUE)
plot(ACF,type="h", xlab="LAG", ylim=c(-.1,1)); abline(h=0)
plot(PACF, type="h", xlab="LAG", ylim=c(-.1,1)); abline(h=0)
```

* Theoretical ACF tails off at lag 12, 24, 36, ...
* Theoretical PACF cut-off at lag 12

```{r results='hide'}
acf2(sAR, max.lag = 35)
```

* Those patterns are also observed in the sample ACF and PACF.

___

<mark>Example 3.47 (p.145) seasonal ARMA</mark>

$$X_t-.8X_{t-12}=W_t-.5W_{t-1}$$

```{r}
phi = c(rep(0,11),.8)
ACF = ARMAacf(ar=phi, ma=-.5, 50)[-1] # [-1] removes 0 lag
PACF = ARMAacf(ar=phi, ma=-.5, 50, pacf=TRUE)
par(mfrow=c(1,2))
plot(ACF, type="h", xlab="LAG", ylim=c(-.4,.8)); abline(h=0)
axis(1, at=0:50, label=F); axis(1, at=seq(0,50,by=10), lwd=2)
plot(PACF, type="h", xlab="LAG", ylim=c(-.4,.8)); abline(h=0)
axis(1, at=0:50, label=F); axis(1, at=seq(0,50,by=10), lwd=2)
```

* PACF cut-off at lag 12, so $AR(1)_{12}$ is suitable.
* ACF cut-off at lag 1, so $MA(1)$ is suitable.

________________

## Periodogram
<mark>Example 4.1 (p.167) A Periodic Series</mark>
```{r}
x1 = 2*cos(2*pi*1:100*6/100) + 3*sin(2*pi*1:100*6/100)
x2 = 4*cos(2*pi*1:100*10/100) + 5*sin(2*pi*1:100*10/100)
x3 = 6*cos(2*pi*1:100*40/100) + 7*sin(2*pi*1:100*40/100)
x = x1 + x2 + x3

par(mfrow=c(2,2), mar=c(4,4,2,1))
plot.ts(x1, ylim=c(-10,10), main=expression(omega==6/100~~~A^2==13))
plot.ts(x2, ylim=c(-10,10), main=expression(omega==10/100~~~A^2==41))
plot.ts(x3, ylim=c(-10,10), main=expression(omega==40/100~~~A^2==85))
plot.ts(x, ylim=c(-16,16), main="sum")
```

when there is many periodic, it is hard to see

* A: height of the peak
  * for $x_1$, $A^2=2^2+3^2=13$, the maximum and minimum values that $x_1$ will attain are $\pm\sqrt{13}=\pm3.61$
* $\omega=\frac jn$: j = repeated times

___

<mark>Example 4.2 (p.169) Estimation and the Periodogram</mark>
```{r}
P = Mod(2*fft(x)/100)^2; Fr = 0:99/100
plot(Fr, P, type="o", xlab="frequency", ylab="scaled periodogram")
abline(v=0.5, col="red", lty=2)
points(x=Fr[7], y=P[7], col="red", pch=1, cex=3);text(x=Fr[7], y=P[7], labels = "6/100", pos=3, col="red")
points(x=Fr[11], y=P[11], col="red", pch=1, cex=3);text(x=Fr[11], y=P[11], labels = "10/100", pos=3, col="red")
points(x=Fr[41], y=P[41], col="red", pch=1, cex=3);text(x=Fr[41], y=P[41], labels = "40/100", pos=1, col="red")
```

* the plot is symmetric: $I(\omega_j)=I(\omega_{n-j})$
* period: $T=\frac nj$
  * $T_1=\frac {100}6$, $T_2=\frac{100}{10}$, $T_3=\frac{100}{40}$
* $\omega_j=\frac jn$
  * $\omega_1=\frac 6{100}$, $\omega_2=\frac {10}{100}$, $\omega_3=\frac {40}{100}$

___

<mark>Example 4.7 (p.176) spectral density </mark>
```{r}
par(mfrow=c(3,1), mar=c(4,4,1,1))
arma.spec(main="White Noise")
arma.spec(ma=.5, main="Moving Average")
arma.spec(ar=c(1,-.9), main="Autoregression")
```

* Top: White Noise
* Middle: MA(1), $\theta_1=0.5$
* Bottom: AR(2), $\phi_1=1, \phi_2=-0.9$

___

<mark>Example 4.10 (p.182) Spectral ANOVA</mark>

> periodogram is symmetric

$$\omega_1=\frac15,\omega_2=\frac25$$

$$SS=2\cdot I(\omega_j)$$

```{r}
x = c(1, 2, 3, 2, 1)
c1 = cos(2*pi*1:5*1/5); s1 = sin(2*pi*1:5*1/5)
c2 = cos(2*pi*1:5*2/5); s2 = sin(2*pi*1:5*2/5)
omega1 = cbind(c1, s1); omega2 = cbind(c2, s2)
anova(lm(x~omega1+omega2)) # ANOVA Table
```

* $2.74=2\cdot I(\frac15)=2\cdot1.37$

$\downarrow$ $I(0)$ $I(\frac15)$ $I(\frac25)$ $I(\frac35)$ $I(\frac45)$
```{r}
Mod(fft(x))^2/5 # the periodogram (as a check)
```

___

<mark>Example 4.13 (p.187) CI </mark>

* n=480, $\because$ periodogram is symmatric, only need to print half of them
* 1 unit = 40

```{r}
par(mfrow=c(2,1), mar=c(4,4,1,1))
soi.per = mvspec(soi, log="no")
abline(v=1/4, lty=2)
rec.per = mvspec(rec, log="no")
abline(v=1/4, lty=2)
```

* the peak appear at j=10, 40

___

$$\frac{2I(\omega_j)}{f(\omega_j)}\sim\chi^2(2)$$

$$\text{95% CI for }f(\omega_j)=[\frac{2I(\omega_j)}{\chi^2_.975}, \frac{2I(\omega_j)}{\chi^2_.025}]$$

* $\downarrow I(\omega_j)$

```{r results='hold'}
soi.per$spec[40] # 0.97223; soi pgram at freq 1/12 = 40/480
soi.per$spec[10] # 0.05372; soi pgram at freq 1/48 = 10/480
```

* $\downarrow CI$: huge interval is a useless interval

```{r results='hold'}
U = qchisq(.025,2) # 0.05063
L = qchisq(.975,2) # 7.37775

2*soi.per$spec[10]/L # 0.01456
2*soi.per$spec[10]/U # 2.12220

2*soi.per$spec[40]/L # 0.26355
2*soi.per$spec[40]/U # 38.40108
```

