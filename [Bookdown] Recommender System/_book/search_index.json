[["index.html", "Recommender System About", " Recommender System ZIH-YING, LI About Personal note for Recommender System Course Item-based Association, Market basket models Frequent item Rating-based: 資料含使用者評分 Content filtering: user獨立 Collaborative filtering: 分群，補缺值，推高評分的item給user Hybrid Models: 組合預測，n個模型的結果平均起來，有點像隨機森林 Model-based Knowledge-based Reference Book Charu C. Aggarwal, Recommender Systems: The Textbook. Using package in R 推薦系統 關聯規則: arules, arulesViz FP-Growth: rCBA (因為arules::fim4r用不了) 協同過濾: recommenderlab 其他 調整圖形輸出: ggplot2 多圖排列: cowplot, gridExtra html表格(kable)調整: kableExtra html表格(kable)映射(mapping): huxtable 資料處理: dplyr Reference Data MovieLens Conjoint Analysis Conjoint Analysis in 10 minutes - Business Performance Management Conjoint Analysis in R: A Marketing Data Science Coding Demonstration How to Manually Calculate Partworth Utilities Association Rule Association mining — Leverage in Python R_programming - (7)關聯式規則(Association rule) 處理多餘的規則、規則右手邊顯示的特徵: Machine Learning -關聯分析-Apriori演算法-詳細解說啤酒與尿布的背後原理 Python實作-Scikit Learn一步一步教學 Introduction to arules – A computational environment for mining association rules and frequent item sets Apriori, Eclat, FP-Growth Collaborative Filtering recommenderlab套件介紹 ___ markdown, bookdown 基本操作: R Markdown 用法 矩陣呈現: Print matrices as tables by default "],["note.html", "Note", " Note Markdown 使用”#“或是”=====“表示h1標題，會顯示在目錄上 如果希望有標題，但不希望它出現在目錄上，可以使用HTML的tag: cross reference Ref: https://bookdown.org/yihui/rmarkdown-cookbook/cross-ref.html https://bookdown.org/yihui/bookdown/cross-references.html 使用\\@ref(type:label) 圖片和表格都要給他們caption，不然會顯示”??” 頁面可以用[Section header text]、[link text][Section header text]或是[link text](#ID) title 的 id 要用{}框起來: {#ID} S3 &amp; S4 class Class S3 S4 Accessing the slot obj$slot obj@slot or slot(obj, \"slot\") check all slots names(obj) slotNames(obj) image image() 會把數值重新調整為0~1的範圍 (Add text and line to an image() in graphics) bookdown render 原則上會把project裡的所有.rmd檔都編成書的內容，順序依照檔案名稱 如果有不想編進去的檔案，或是要調整順序，則要在_bookdown.yml裡增加 rmd_files: [&quot;index.Rmd&quot;, &quot;02-literature.Rmd&quot;, &quot;01-intro.Rmd&quot;] 編譯整本書通常會很花時間，如果只是想預覽部分頁面可以使用 bookdown::preview_chapter(\"file name.Rmd\") Theme of gitbook Ref: https://rstudio4edu.github.io/rstudio4edu-book/book-dress.html 也可以自己用css調整 default、tango、pygments、kate 只適合 white monochrome 和 haddock 函數高亮不明顯 / 沒有 適合 night 的程式碼格式 (放在_output.yml) highlight: espresso highlight: zenburn "],["problem.html", "Problem", " Problem arules::fim4r安裝問題 ([compare different algorithm] mine rule of AdultUCI data set) 在第一次使用fim4r時會要求安裝，回答是或否以繼續。我好像沒辦法用，看起來是安裝路徑的問題，但是它要求安裝時沒有其他要求輸入的東西。 另外這個”C:\\U”的路徑問題是自從某次開RStudio後就有的，重新安裝也沒用，搜尋不到解決方法，替代方案是利用file.edit(\".Rprofile\")在工作目錄建立一個.Rprofile，再重啟R就不會有這個路徑問題。 arules::apriori中的appearance問題 (早餐店購買紀錄) 在早餐店購買紀錄的練習中，要注意關聯的右手邊不會有”性別”和”是否為學生”，因為這不是能被買甚麼早餐決定的，因此要設定appearance的參數。 但是經過設定後，被appearance使用到的變數不會出現在另一邊，詳細見練習題最後面的Notice部分 "],["conjoint-analysis.html", "Sec 1 Conjoint Analysis 1.1 Example", " Sec 1 Conjoint Analysis 聯合分析 (Conjoint Analysis) 或稱偏好分析，常用於社會科學或是市場分析等，基於特徵組合(產品特色)的排名，可以幫助我們把特徵組合分解成每個單獨特徵的偏好(i.e. part-worth, 部分價值)。 不只可以應用於產品 (品質 or 價格)，也可應用於服務 (良好的服務 or 較短的等待時間 or 較低的價格)。 對於企業來說，準確了解客戶如何評價產品或服務的不同元素意味著產品或服務的部署可以變得更加容易，並且可以在更大程度上進行最佳化 Simple Introduction Example 有兩個mp3，分別為A: 16MB,運送時間1天；B: 64MB,運送時間1週，偏好哪一個? attr. 1: memory attr. 2: delivery 如果選擇A，表示更強調短運送時間；選擇B則更強調大記憶體 Relative importance 計算每個個體在各特徵下的偏好範圍(preference range) 計算每個個體在各特徵下的重要比率(importance ratio) 計算個特徵的平均重要性(average importance) part-worth 計算各水準(level)下個體間的平均偏好(average preference) 標準化至\\(\\mu=0\\) Use linear regression \\[Ranking=\\mu+\\beta_1Attr_1+\\beta_2Attr_2\\] \\(\\beta\\)表示part-worth，所有類別的加總須為0 if \\(Attr_i\\) 只有兩類，則\\[relative\\:importance=\\pm\\beta_i/\\sum \\pm\\beta_i\\] 假設\\(\\beta_1=2\\)，\\(\\beta_2=0.5\\)，則\\(\\sum \\pm\\beta_i=4+1=5\\) memory的relative importance\\(=4/5=80\\%\\) delivery的relative importance\\(=1/5=20\\%\\) 1.1 Example 1.1.1 Preprocess Read data conjoint.data.frame &lt;- read.csv(&quot;data/mobile_services_ranking.csv&quot;) head(conjoint.data.frame, 3) |&gt; knitr::kable() brand startup monthly service retail apple samsung google ranking “AT&amp;T” “$100” “$100” “4G NO” “Retail NO” “Apple NO” “Samsung NO” “Nexus NO” 11 “Verizon” “$300” “$100” “4G NO” “Retail YES” “Apple YES” “Samsung YES” “Nexus NO” 12 “US Cellular” “$400” “$200” “4G NO” “Retail NO” “Apple NO” “Samsung YES” “Nexus NO” 9 Fit linear model using main effects only (no interaction terms) main.effects.model &lt;- {ranking ~ brand + startup + monthly + service + retail + apple + samsung + google} main.effects.model.fit &lt;- lm(main.effects.model, data=conjoint.data.frame) summary(main.effects.model.fit)$coef |&gt; round(2) |&gt; knitr::kable() Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 11.12 0.48 22.98 0.03 brand”T-Mobile” -0.25 0.35 -0.71 0.61 brand”US Cellular” 0.00 0.35 0.00 1.00 brand”Verizon” 0.25 0.35 0.71 0.61 startup”$200” -0.75 0.35 -2.12 0.28 startup”$300” -0.75 0.35 -2.12 0.28 startup”$400” -1.50 0.35 -4.24 0.15 monthly”$200” -3.00 0.35 -8.49 0.07 monthly”$300” -6.25 0.35 -17.68 0.04 monthly”$400” -10.75 0.35 -30.41 0.02 service”4G YES” 3.50 0.25 14.00 0.05 retail”Retail YES” -0.50 0.25 -2.00 0.30 apple”Apple YES” -0.50 0.25 -2.00 0.30 samsung”Samsung YES” 2.25 0.25 9.00 0.07 google”Nexus YES” 1.50 0.25 6.00 0.11 sum(coef(main.effects.model.fit)[-1]) ## [1] -16.75 Store result save key list elements of the fitted model as needed for conjoint measures conjoint.results &lt;- main.effects.model.fit[c(&quot;contrasts&quot;,&quot;xlevels&quot;,&quot;coefficients&quot;)] conjoint.results$attributes &lt;- names(conjoint.results$contrasts) conjoint.results ## $contrasts ## $contrasts$brand ## [1] &quot;contr.treatment&quot; ## ## $contrasts$startup ## [1] &quot;contr.treatment&quot; ## ## $contrasts$monthly ## [1] &quot;contr.treatment&quot; ## ## $contrasts$service ## [1] &quot;contr.treatment&quot; ## ## $contrasts$retail ## [1] &quot;contr.treatment&quot; ## ## $contrasts$apple ## [1] &quot;contr.treatment&quot; ## ## $contrasts$samsung ## [1] &quot;contr.treatment&quot; ## ## $contrasts$google ## [1] &quot;contr.treatment&quot; ## ## ## $xlevels ## $xlevels$brand ## [1] &quot;\\&quot;AT&amp;T\\&quot;&quot; &quot;\\&quot;T-Mobile\\&quot;&quot; &quot;\\&quot;US Cellular\\&quot;&quot; &quot;\\&quot;Verizon\\&quot;&quot; ## ## $xlevels$startup ## [1] &quot;\\&quot;$100\\&quot;&quot; &quot;\\&quot;$200\\&quot;&quot; &quot;\\&quot;$300\\&quot;&quot; &quot;\\&quot;$400\\&quot;&quot; ## ## $xlevels$monthly ## [1] &quot;\\&quot;$100\\&quot;&quot; &quot;\\&quot;$200\\&quot;&quot; &quot;\\&quot;$300\\&quot;&quot; &quot;\\&quot;$400\\&quot;&quot; ## ## $xlevels$service ## [1] &quot;\\&quot;4G NO\\&quot;&quot; &quot;\\&quot;4G YES\\&quot;&quot; ## ## $xlevels$retail ## [1] &quot;\\&quot;Retail NO\\&quot;&quot; &quot;\\&quot;Retail YES\\&quot;&quot; ## ## $xlevels$apple ## [1] &quot;\\&quot;Apple NO\\&quot;&quot; &quot;\\&quot;Apple YES\\&quot;&quot; ## ## $xlevels$samsung ## [1] &quot;\\&quot;Samsung NO\\&quot;&quot; &quot;\\&quot;Samsung YES\\&quot;&quot; ## ## $xlevels$google ## [1] &quot;\\&quot;Nexus NO\\&quot;&quot; &quot;\\&quot;Nexus YES\\&quot;&quot; ## ## ## $coefficients ## (Intercept) brand&quot;T-Mobile&quot; brand&quot;US Cellular&quot; ## 1.112500e+01 -2.500000e-01 -6.628732e-16 ## brand&quot;Verizon&quot; startup&quot;$200&quot; startup&quot;$300&quot; ## 2.500000e-01 -7.500000e-01 -7.500000e-01 ## startup&quot;$400&quot; monthly&quot;$200&quot; monthly&quot;$300&quot; ## -1.500000e+00 -3.000000e+00 -6.250000e+00 ## monthly&quot;$400&quot; service&quot;4G YES&quot; retail&quot;Retail YES&quot; ## -1.075000e+01 3.500000e+00 -5.000000e-01 ## apple&quot;Apple YES&quot; samsung&quot;Samsung YES&quot; google&quot;Nexus YES&quot; ## -5.000000e-01 2.250000e+00 1.500000e+00 ## ## $attributes ## [1] &quot;brand&quot; &quot;startup&quot; &quot;monthly&quot; &quot;service&quot; &quot;retail&quot; &quot;apple&quot; &quot;samsung&quot; ## [8] &quot;google&quot; 1.1.2 Compute part-worth last.part.worth: \\(-\\sum \\hat{\\beta}\\)， 除了base類別以外的類別的係數，例如顏色分紅黃藍，base為紅色，則\\(\\sum \\hat{\\beta}=\\)黃色加藍色的係數 part.worths: 所有類別的part-worth，加總為0 part.worths &lt;- conjoint.results$xlevels # list of same structure as xlevels end.index.for.coefficient &lt;- 1 # initialize skipping the intercept part.worth.vector &lt;- NULL # used for accumulation of part worths for(index.for.attribute in seq(along=conjoint.results$contrasts)) { nlevels &lt;- length(unlist(conjoint.results$xlevels[index.for.attribute])) begin.index.for.coefficient &lt;- end.index.for.coefficient + 1 end.index.for.coefficient &lt;- begin.index.for.coefficient + nlevels -2 last.part.worth &lt;- -sum(conjoint.results$coefficients[begin.index.for.coefficient:end.index.for.coefficient]) part.worths[index.for.attribute] &lt;- list(as.numeric(c(conjoint.results$coefficients[begin.index.for.coefficient:end.index.for.coefficient], last.part.worth))) part.worth.vector &lt;- c(part.worth.vector,unlist(part.worths[index.for.attribute])) } conjoint.results$part.worths &lt;- part.worths seq(along=conjoint.results$contrasts) seq(c(3,4)) ## [1] 1 2 seq_along(c(3,4)) ## [1] 1 2 seq(along=c(3,4)) ## [1] 1 2 seq(5) ## [1] 1 2 3 4 5 seq_along(5) ## [1] 1 seq(along=5) ## [1] 1 標準化，讓每個類別的基準值位於0 standardize &lt;- function(x) {(x - mean(x)) / sd(x)} conjoint.results$standardized.part.worths &lt;- lapply(conjoint.results$part.worths,standardize) head(part.worths, 3) ## $brand ## [1] -2.500000e-01 -6.628732e-16 2.500000e-01 -2.140445e-15 ## ## $startup ## [1] -0.75 -0.75 -1.50 3.00 ## ## $monthly ## [1] -3.00 -6.25 -10.75 20.00 head(conjoint.results$standardized.part.worths, 3) ## $brand ## [1] -1.224745e+00 -3.247402e-15 1.224745e+00 -1.048600e-14 ## ## $startup ## [1] -0.3692745 -0.3692745 -0.7385489 1.4770979 ## ## $monthly ## [1] -0.2188703 -0.4559797 -0.7842851 1.4591351 1.1.3 Compute relative importance 計算各特徵的全距並加總 part.worth.ranges &lt;- conjoint.results$contrasts for(index.for.attribute in seq(along=conjoint.results$contrasts)) part.worth.ranges[index.for.attribute] &lt;- dist(range(conjoint.results$part.worths[index.for.attribute])) conjoint.results$part.worth.ranges &lt;- part.worth.ranges sum.part.worth.ranges &lt;- sum(as.numeric(conjoint.results$part.worth.ranges)) 計算各特徵的全距占所有特徵的比例 attribute.importance &lt;- conjoint.results$contrasts for(index.for.attribute in seq(along=conjoint.results$contrasts)) attribute.importance[index.for.attribute] &lt;- (as.numeric(part.worth.ranges[index.for.attribute])/sum.part.worth.ranges) * 100 conjoint.results$attribute.importance &lt;- attribute.importance data.frame(importance=unlist(attribute.importance)) |&gt; knitr::kable() importance brand 0.9569378 startup 8.6124402 monthly 58.8516746 service 13.3971292 retail 1.9138756 apple 1.9138756 samsung 8.6124402 google 5.7416268 1.1.4 Output 1.1.4.1 Setting 排序 &amp; \\(R^2\\) attribute.name &lt;- names(conjoint.results$contrasts) attribute.importance &lt;- as.numeric(attribute.importance) temp.frame &lt;- data.frame(attribute.name,attribute.importance) conjoint.results$ordered.attributes &lt;- as.character(temp.frame[sort.list(temp.frame$attribute.importance,decreasing = TRUE), &quot;attribute.name&quot;]) # respondent internal consistency added to list structure conjoint.results$internal.consistency &lt;- summary(main.effects.model.fit)$r.squared 呈現位數 print.digits &lt;- 2 if (print.digits == 2) pretty.print &lt;- function(x) {sprintf(&quot;%1.2f&quot;,round(x,digits = 2))} if (print.digits == 3) pretty.print &lt;- function(x) {sprintf(&quot;%1.3f&quot;,round(x,digits = 3))} 顯示欄位名稱 (圖形用的) effect.name.map &lt;- function(effect.name) { if(effect.name==&quot;brand&quot;) return(&quot;Mobile Service Provider&quot;) if(effect.name==&quot;startup&quot;) return(&quot;Start-up Cost&quot;) if(effect.name==&quot;monthly&quot;) return(&quot;Monthly Cost&quot;) if(effect.name==&quot;service&quot;) return(&quot;Offers 4G Service&quot;) if(effect.name==&quot;retail&quot;) return(&quot;Has Nearby Retail Store&quot;) if(effect.name==&quot;apple&quot;) return(&quot;Sells Apple Products&quot;) if(effect.name==&quot;samsung&quot;) return(&quot;Sells Samsung Products&quot;) if(effect.name==&quot;google&quot;) return(&quot;Sells Google/Nexus Products&quot;) } options設定 (好像沒差?) # # set up sum contrasts for effects coding as needed for conjoint analysis # options(contrasts=c(&quot;contr.sum&quot;,&quot;contr.poly&quot;)) 1.1.4.2 Report for(k in seq(along=conjoint.results$ordered.attributes)) { cat(&quot;\\n&quot;,&quot;\\n&quot;) cat(conjoint.results$ordered.attributes[k],&quot;Levels: &quot;, unlist(conjoint.results$xlevels[conjoint.results$ordered.attributes[k]])) cat(&quot;\\n&quot;,&quot; Part-Worths: &quot;) cat(pretty.print(unlist(conjoint.results$part.worths [conjoint.results$ordered.attributes[k]]))) cat(&quot;\\n&quot;,&quot; Standardized Part-Worths: &quot;) cat(pretty.print(unlist(conjoint.results$standardized.part.worths [conjoint.results$ordered.attributes[k]]))) cat(&quot;\\n&quot;,&quot; Attribute Importance: &quot;) cat(pretty.print(unlist(conjoint.results$attribute.importance [conjoint.results$ordered.attributes[k]]))) } ## ## ## monthly Levels: &quot;$100&quot; &quot;$200&quot; &quot;$300&quot; &quot;$400&quot; ## Part-Worths: -3.00 -6.25 -10.75 20.00 ## Standardized Part-Worths: -0.22 -0.46 -0.78 1.46 ## Attribute Importance: 58.85 ## ## service Levels: &quot;4G NO&quot; &quot;4G YES&quot; ## Part-Worths: 3.50 -3.50 ## Standardized Part-Worths: 0.71 -0.71 ## Attribute Importance: 13.40 ## ## startup Levels: &quot;$100&quot; &quot;$200&quot; &quot;$300&quot; &quot;$400&quot; ## Part-Worths: -0.75 -0.75 -1.50 3.00 ## Standardized Part-Worths: -0.37 -0.37 -0.74 1.48 ## Attribute Importance: 8.61 ## ## samsung Levels: &quot;Samsung NO&quot; &quot;Samsung YES&quot; ## Part-Worths: 2.25 -2.25 ## Standardized Part-Worths: 0.71 -0.71 ## Attribute Importance: 8.61 ## ## google Levels: &quot;Nexus NO&quot; &quot;Nexus YES&quot; ## Part-Worths: 1.50 -1.50 ## Standardized Part-Worths: 0.71 -0.71 ## Attribute Importance: 5.74 ## ## apple Levels: &quot;Apple NO&quot; &quot;Apple YES&quot; ## Part-Worths: -0.50 0.50 ## Standardized Part-Worths: -0.71 0.71 ## Attribute Importance: 1.91 ## ## retail Levels: &quot;Retail NO&quot; &quot;Retail YES&quot; ## Part-Worths: -0.50 0.50 ## Standardized Part-Worths: -0.71 0.71 ## Attribute Importance: 1.91 ## ## brand Levels: &quot;AT&amp;T&quot; &quot;T-Mobile&quot; &quot;US Cellular&quot; &quot;Verizon&quot; ## Part-Worths: -0.25 -0.00 0.25 -0.00 ## Standardized Part-Worths: -1.22 -0.00 1.22 -0.00 ## Attribute Importance: 0.96 1.1.4.3 Plot source(&quot;R/spine_chart.R&quot;) spine.chart(conjoint.results) "],["self-learning.html", "Sec 2 Self-learning 2.1 Enter new ranking 2.2 Add interaction 2.3 Rewrite spine_chart.R", " Sec 2 Self-learning 2.1 Enter new ranking Enter your own rankings for the product profiles and generate conjoint measures of attribute importance and level part-worths. 隨機分配排名 # get data df &lt;- conjoint.data.frame set.seed(0920) df$ranking &lt;- sample(1:16) # head(df,3); head(conjoint.data.frame,3) # fit model ft &lt;- lm(ranking~., data=df) # get `conjoint.results` new.conjoint.results &lt;- get_result(ft) # report show_report(new.conjoint.results) ## ## ## brand Levels: &quot;AT&amp;T&quot; &quot;T-Mobile&quot; &quot;US Cellular&quot; &quot;Verizon&quot; ## Part-Worths: -1.00 -2.00 -4.00 7.00 ## Standardized Part-Worths: -0.21 -0.41 -0.83 1.45 ## Attribute Importance: 26.35 ## ## startup Levels: &quot;$100&quot; &quot;$200&quot; &quot;$300&quot; &quot;$400&quot; ## Part-Worths: -1.75 1.25 5.50 -5.00 ## Standardized Part-Worths: -0.39 0.28 1.23 -1.12 ## Attribute Importance: 25.15 ## ## monthly Levels: &quot;$100&quot; &quot;$200&quot; &quot;$300&quot; &quot;$400&quot; ## Part-Worths: -2.75 -3.00 3.75 2.00 ## Standardized Part-Worths: -0.81 -0.88 1.10 0.59 ## Attribute Importance: 16.17 ## ## retail Levels: &quot;Retail NO&quot; &quot;Retail YES&quot; ## Part-Worths: -2.25 2.25 ## Standardized Part-Worths: -0.71 0.71 ## Attribute Importance: 10.78 ## ## samsung Levels: &quot;Samsung NO&quot; &quot;Samsung YES&quot; ## Part-Worths: 1.50 -1.50 ## Standardized Part-Worths: 0.71 -0.71 ## Attribute Importance: 7.19 ## ## apple Levels: &quot;Apple NO&quot; &quot;Apple YES&quot; ## Part-Worths: -1.25 1.25 ## Standardized Part-Worths: -0.71 0.71 ## Attribute Importance: 5.99 ## ## service Levels: &quot;4G NO&quot; &quot;4G YES&quot; ## Part-Worths: -1.00 1.00 ## Standardized Part-Worths: -0.71 0.71 ## Attribute Importance: 4.79 ## ## google Levels: &quot;Nexus NO&quot; &quot;Nexus YES&quot; ## Part-Worths: -0.75 0.75 ## Standardized Part-Worths: -0.71 0.71 ## Attribute Importance: 3.59 # plot par(mfrow = c(1, 2), mar=c(1,1,1,1)) spine.chart(new.conjoint.results) title(&#39;New ranking&#39;) spine.chart(conjoint.results) title(&#39;Origin&#39;) Conclusion 隨機生成ranking後，最重要的變為電信商，但是重要性僅26.35% 2.2 Add interaction Beyond a linear main-effects model. See if you can build a model with interaction effects for service provider attributes. 加入samsung和google的交互作用項 head(conjoint.data.frame,3) |&gt; knitr::kable() brand startup monthly service retail apple samsung google ranking “AT&amp;T” “$100” “$100” “4G NO” “Retail NO” “Apple NO” “Samsung NO” “Nexus NO” 11 “Verizon” “$300” “$100” “4G NO” “Retail YES” “Apple YES” “Samsung YES” “Nexus NO” 12 “US Cellular” “$400” “$200” “4G NO” “Retail NO” “Apple NO” “Samsung YES” “Nexus NO” 9 # fit model ft_interaction &lt;- lm(ranking~. + samsung*google, data=conjoint.data.frame) # ft_interaction summary(ft_interaction) ## ## Call: ## lm(formula = ranking ~ . + samsung * google, data = conjoint.data.frame) ## ## Residuals: ## ALL 16 residuals are 0: no residual degrees of freedom! ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.100e+01 NaN NaN NaN ## brand&quot;T-Mobile&quot; -2.500e-01 NaN NaN NaN ## brand&quot;US Cellular&quot; -6.459e-16 NaN NaN NaN ## brand&quot;Verizon&quot; 2.500e-01 NaN NaN NaN ## startup&quot;$200&quot; -7.500e-01 NaN NaN NaN ## startup&quot;$300&quot; -7.500e-01 NaN NaN NaN ## startup&quot;$400&quot; -1.500e+00 NaN NaN NaN ## monthly&quot;$200&quot; -3.000e+00 NaN NaN NaN ## monthly&quot;$300&quot; -6.250e+00 NaN NaN NaN ## monthly&quot;$400&quot; -1.075e+01 NaN NaN NaN ## service&quot;4G YES&quot; 3.500e+00 NaN NaN NaN ## retail&quot;Retail YES&quot; -5.000e-01 NaN NaN NaN ## apple&quot;Apple YES&quot; -5.000e-01 NaN NaN NaN ## samsung&quot;Samsung YES&quot; 2.500e+00 NaN NaN NaN ## google&quot;Nexus YES&quot; 1.750e+00 NaN NaN NaN ## samsung&quot;Samsung YES&quot;:google&quot;Nexus YES&quot; -5.000e-01 NaN NaN NaN ## ## Residual standard error: NaN on 0 degrees of freedom ## Multiple R-squared: 1, Adjusted R-squared: NaN ## F-statistic: NaN on 15 and 0 DF, p-value: NA # get `conjoint.results` interaction.conjoint.results &lt;- get_result(ft_interaction) # report show_report(interaction.conjoint.results) ## ## ## monthly Levels: &quot;$100&quot; &quot;$200&quot; &quot;$300&quot; &quot;$400&quot; ## Part-Worths: -3.00 -6.25 -10.75 20.00 ## Standardized Part-Worths: -0.22 -0.46 -0.78 1.46 ## Attribute Importance: 57.75 ## ## service Levels: &quot;4G NO&quot; &quot;4G YES&quot; ## Part-Worths: 3.50 -3.50 ## Standardized Part-Worths: 0.71 -0.71 ## Attribute Importance: 13.15 ## ## samsung Levels: &quot;Samsung NO&quot; &quot;Samsung YES&quot; ## Part-Worths: 2.50 -2.50 ## Standardized Part-Worths: 0.71 -0.71 ## Attribute Importance: 9.39 ## ## startup Levels: &quot;$100&quot; &quot;$200&quot; &quot;$300&quot; &quot;$400&quot; ## Part-Worths: -0.75 -0.75 -1.50 3.00 ## Standardized Part-Worths: -0.37 -0.37 -0.74 1.48 ## Attribute Importance: 8.45 ## ## google Levels: &quot;Nexus NO&quot; &quot;Nexus YES&quot; ## Part-Worths: 1.75 -1.75 ## Standardized Part-Worths: 0.71 -0.71 ## Attribute Importance: 6.57 ## ## apple Levels: &quot;Apple NO&quot; &quot;Apple YES&quot; ## Part-Worths: -0.50 0.50 ## Standardized Part-Worths: -0.71 0.71 ## Attribute Importance: 1.88 ## ## retail Levels: &quot;Retail NO&quot; &quot;Retail YES&quot; ## Part-Worths: -0.50 0.50 ## Standardized Part-Worths: -0.71 0.71 ## Attribute Importance: 1.88 ## ## brand Levels: &quot;AT&amp;T&quot; &quot;T-Mobile&quot; &quot;US Cellular&quot; &quot;Verizon&quot; ## Part-Worths: -0.25 -0.00 0.25 -0.00 ## Standardized Part-Worths: -1.22 -0.00 1.22 -0.00 ## Attribute Importance: 0.94 # plot par(mfrow = c(1, 2), mar=c(1,1,1,1)) spine.chart(interaction.conjoint.results) title(&#39;Add interaction&#39;) spine.chart(conjoint.results) title(&#39;Origin&#39;) Conclusion 加入samsung和google的交互作用項 samsung的係數: \\(2.25\\to2.5\\) google的係數: \\(1.5\\to1.75\\) samsung*google的係數為: \\(-0.5\\)，表示當samsung和google都有代理時會降低喜好的排名 2.3 Rewrite spine_chart.R See if you can rewrite the source script spine_chart.R to generalize the spine chart with more flexibility (skip) "],["association-rule.html", "Sec 3 Association Rule 3.1 Evaluation Metrics 3.2 Example: DVD 3.3 Example: Income 3.4 [處理多餘規則] Example: Groceries", " Sec 3 Association Rule 關聯規則(Association Rule)的目標是找出大型資料集中經常共同出現的物件組合，以及這個組合中物件的關聯性。 所以關聯規則中要找的分別是: 頻繁項集(Frequent Itemsets): 經常一起出現的物品集合 關聯規則(Association Rules): 表達物品之間可能存在強烈的關聯性 在計算關聯規則中要定義兩個門檻來篩選得到的規則: 最小支持度(min Support): 去除不常見的 最小信賴度(min Confidence): 去除正確率低的 關聯規則中最基本的演算法分別有3個: Apriori, Eclat 和 FP-Growth Apriori Eclat 運算速度 慢 快 搜尋 廣度優先 深度優先 Some Note apriori的進度顯示 在不給參數的情況下使用apriori會顯示個步驟的運行時間，如果不想顯示它可以用下列方法 使用markdown的話可以在code chunk加入results='hide'來隱藏結果 或是在執行apriori時加入control = list(verbose=F)的參數 Display results for html format Tables 以表格呈現探勘出的規則 arules::inspect(): 以程式碼結果呈現，適合呈現寬度較窄的結果，較難調整輸出。 只有這個可以用linebreak讓較長的規則容易比較 inspect(rule) arulse::DATAFRAME(): 以表格呈現，適合只列出少數規則時使用，可以透過format調整輸出 DATAFRAME(rule) |&gt; format(digits=2) |&gt; knitr::kable() DATAFRAME(rule, separate=F): LHS, RHS 放同一欄位 arulesViz::inspectDT(): 以DT table呈現，適合想列出全部的規則時使用，可以透過DT::table相關程式調整輸出，預設會把篩選器(filter)放在表頭下面表格內容上面(filter='none'沒辦法移出它) inspectDT(rule) |&gt; DT::formatRound(columns='count', digits=0) Matrix pander::pander(): pander套件可以用更好看的方式呈現矩陣，超出頁面範圍會自動截斷放到下面，因此也不適合大矩陣 Plots 探索資料 image(): 以矩陣圖顯示，只適合呈現小資料 arules::itemFrequencyPlot(): 以柱狀圖顯示交易物件中各項目的次數 詳細見 Income 範例 探索關聯規則，詳細內容及範例結果見 DVD 範例 plot(rule, measure=c(\"x\",\"y\"), shading=\"col\",control=list(jitter=6)): 呈現每條規則的3種指標，預設繪圖引擎是ggplot，可以透過ggplot2套件中的函數調整結果 x, y, col: support, confidence, lift, order(規則的順序) 透過ggplot2函數調整: e.g. + geom_point(size=3) plot(rule, method=\"grouped\"): 呈現LHS和RHS之間的lift和support 透過control=list(): e.g. control=list(k=10) plot(rule, method=\"graph\"): 呈現節點之間的lift和support 3.1 Evaluation Metrics 常用指標 Support 支持度: 衡量關聯規則正確性 \\(\\to\\) 具有一定普遍性 \\[ Support(X\\to Y)=P(X\\cap Y)=Support(Y\\to X)\\\\ \\color{red}{\\text{不一定用機率，也可以是次數}} \\] Confidence 信賴度: 衡量關聯規則顯著性 非因果關係，而是換個角度看 \\(P(Y|X)\\approx P(X|Y)\\Rightarrow\\) 互補品 \\(\\to\\) 無參考價值 \\[ Confidence(X\\to Y)=P(Y|X)=\\frac{P(X\\cap Y)}{P(X)}=\\frac{Support(X\\to Y)}{P(X)} \\] Lift 增益度: 衡量關聯規則資訊價值 縮小定義域後的有效性 \\(Lift&gt;1\\to P(Y|X)&gt;P(Y)\\to\\) 該關聯預測好 \\(\\to\\) 正相關 \\(Lift&lt;1\\to P(Y|X)&lt;P(Y)\\to\\) 該關聯預測差 \\(\\to\\) 負相關 \\[ Lift(X\\to Y)=\\frac{Confidence(X\\to Y)}{P(Y)}=\\frac{P(Y|X)}{P(Y)}=\\frac{P(X\\cap Y)}{P(X)\\color{red}{P(Y)}} \\] 其他指標 Added value 附加價值 \\[ \\begin{array}{ll} AV(X\\to Y)&amp;=\\frac{supp(X\\to Y)}{supp(X)}-supp(Y)\\\\&amp;=conf(X\\to Y)-supp(Y) \\end{array} \\] Conviction 信念 \\[CV(X\\to Y)=\\frac{1-supp(Y)}{1-conf(X\\to Y)}\\] Leverage: 跟lift相比，lift用的是比率，leverage用的是差(difference) \\(Leverage=0\\Rightarrow P(X\\cap Y)=P(X)P(Y)\\to\\) A, B獨立(independent) \\(Leverage&gt;1\\Rightarrow P(X\\cap Y)&gt;P(X)P(Y)\\to\\) 正相關，觀察支持度高於期望支持度 \\(Leverage&lt;1\\Rightarrow P(X\\cap Y)&lt;P(X)P(Y)\\to\\) 負相關，X, Y不會同時出現 \\[ \\begin{array}{ll} Leverage(X\\to Y)&amp;=supp(X\\to Y)-supp(X)\\cdot supp(Y)\\\\ &amp;=P(X\\cap Y)-P(X)P(Y)\\\\ Lift(X\\to Y)&amp;=\\frac{P(X\\cap Y)}{P(X)P(Y)} \\end{array} \\] 3.2 Example: DVD 3.2.1 Prepare data library(arules) library(arulesViz) library(ggplot2) # for changing plot option dat0=read.csv(&quot;data/dvdtrans.csv&quot;) str(dat0) ## &#39;data.frame&#39;: 30 obs. of 2 variables: ## $ ID : int 1 1 1 1 1 2 2 2 3 3 ... ## $ Item: chr &quot;Sixth Sense&quot; &quot;LOTR1&quot; &quot;Harry Potter1&quot; &quot;Green Mile&quot; ... factor(dat0$Item) |&gt; levels() ## [1] &quot;Braveheart&quot; &quot;Gladiator&quot; &quot;Green Mile&quot; &quot;Harry Potter1&quot; ## [5] &quot;Harry Potter2&quot; &quot;LOTR&quot; &quot;LOTR1&quot; &quot;LOTR2&quot; ## [9] &quot;Patriot&quot; &quot;Sixth Sense&quot; split(x, f, ...) x: 要被分組的東西 f: 用甚麼分 dat=split(as.factor(dat0[,&quot;Item&quot;]),as.factor(dat0[,&quot;ID&quot;])) knitr::kable(list(dat[[1]], dat0[dat0$ID==1,])) x Sixth Sense LOTR1 Harry Potter1 Green Mile LOTR2 ID Item 1 Sixth Sense 1 LOTR1 1 Harry Potter1 1 Green Mile 1 LOTR2 itemMatrix vs. transactions: 接用於表示關聯規則或交易資料，差別僅在於表示方式 newdata=as(dat, &quot;itemMatrix&quot;) newdata1=as(dat, &quot;transactions&quot;) itemMatrix transactions 表示 矩陣形式 列表形式 e.g. 物件大小 5248 5248 image(x, xlab = &quot;Items (Columns)&quot;, ylab = &quot;Elements (Rows)&quot;, ...) 提供image方法給itemMatrix和transactions物件 x軸為項目 y軸為使用者 image(newdata, main=&quot;10 transactions/customers&quot;, xlab = &#39;Item&#39;, ylab = &#39;ID&#39;) itemFrequencyPlot(x, type = c(&quot;relative&quot;, &quot;absolute&quot;), support = NULL, topN = NULL, lift = FALSE, horiz = FALSE, names = TRUE, cex.names = graphics::par(&quot;cex.axis&quot;), ...) type: 預設呈現的是相對次數 support: 至少多少支持度才顯示 topN: 只顯示topN個最高次數或是增益度(lift=TRUE)的項目 horiz: 圖形轉置 names, cex.names: bar標籤顯示與否、字體大小 itemFrequencyPlot(newdata) # relative freq. 3.2.2 Mining rules 3.2.3 By Apriori get rules apriori(data, parameter = NULL, appearance = NULL, ...): Mine frequent itemsets, association rules or association hyperedges using the Apriori algorithm. data: transactions object parameter: named list or APparameter object default min support 0.1 min confidence 0.8 max length(maxlen) 10 items max time(maxtime) for subset checking of 5 sec. appearance: named list or APparameter object default: all items can appear set right hand side: appearance = list(default=\"lhs\", rhs=c(\"Survived=No\", \"Survived=Yes\")) rhs: 右手邊顯示的特徵 DVD=apriori(newdata, parameter=list(support=0.1,confidence=0.1,maxlen=5)) inspect: display rule inspectDT: display rule by datatable in DT package DATAFRAME: display rule by data.frame # summary(DVD) inspectDT(DVD) |&gt; DT::formatRound(columns=&#39;count&#39;, digits=0) lhs: left hand side rhs: right hand side interestMeasure(x, measure, ...): Calculate Additional Interest Measures x: itemsets or rules measure: name or vector of names of the desired interest measures (see details) DVD@lhs@data@Dim[2] # number of rules ## [1] 127 DT::datatable(interestMeasure(DVD, c(&quot;support&quot;,&quot;confidence&quot;, &quot;lift&quot;)), selection = &#39;none&#39;) 3.2.3.1 plot results remind: \\(lift&gt;1\\to\\text{good after 縮小定義域}\\) \\(lift&lt;1\\to\\text{bad after 縮小定義域}\\) plot(DVD, measure=c(&quot;confidence&quot;,&quot;lift&quot;), shading=&quot;support&quot;,control=list(jitter=6))+ geom_point(size=3) order: 規則的順序 order=1 \\(\\to\\) 10個 LHS為空，RHS分別為10個電影的規則 order=5 \\(\\to\\) 5個 LHS為4部電影的規則 plot(DVD,measure=c(&quot;confidence&quot;,&quot;lift&quot;),shading=&quot;order&quot;,control=list(jitter=6))+ ggplot2::geom_point(size=3) method = grouped: 即grouped matrix，矩陣中的前因(列)使用分群(cluster)進行分組。 組別由組中最有趣的項目(’組中支持度’與’所有規則支持度’之比最高)表示。 控制參數包含 k: 前因組的數量(預設20) \\(\\to\\) x軸 rhs_max: RHS最大數量(預設10) \\(\\to\\) y軸 lhs_items: LHS項目即顯示的數量(預設2) \\(\\to\\) 超過2個以 ‘\\(+x\\) items’ 顯示 (好像沒有了 ## Warning: Unknown control parameters: lhs_items) aggr.fun: 聚合函數可以是從向量計算標量的任何函數(例如，min、mean(預設)、median、sum、max)。 它也用於重新排列繪圖中的氣泡(bubble/ballon)。 plot(DVD,method=&quot;grouped&quot;) method = graph: 項目表示為帶有標籤的節點，規則（或項集）則表示為與項目使用箭頭連接的節點 見3.2.1.3子集範例較清楚 plot(head(sort(DVD,by=&quot;lift&quot;),100),method=&quot;graph&quot;) 3.2.3.2 Check only RHS is “Patriot” arules::subset(x, subset, ...): 用於對滿足某些條件的關聯(associations)或交易(transactions )(itemMatrix)子集進行操作(e.g. 包含特定項目或滿足最小增益度) subset: logical expression indicating elements to keep DVDx &lt;- subset(DVD,subset= rhs %in% &quot;Patriot&quot; &amp; lift&gt;1) DATAFRAME(DVDx) |&gt; format(digits=2) |&gt; knitr::kable() LHS RHS support confidence coverage lift count 13 {Braveheart} {Patriot} 0.1 1.00 0.1 1.7 1 46 {Sixth Sense} {Patriot} 0.4 0.67 0.6 1.1 4 48 {Gladiator} {Patriot} 0.6 0.86 0.7 1.4 6 52 {Braveheart,Gladiator} {Patriot} 0.1 1.00 0.1 1.7 1 98 {Gladiator,Sixth Sense} {Patriot} 0.4 0.80 0.5 1.3 4 移除control=list(type=\"items\"): 因為## Warning: Unknown control parameters: type # plot(sort(DVD,by=&quot;lift&quot;),method=&quot;graph&quot;,control=list(type=&quot;items&quot;)) plot(DVDx,method=&quot;grouped&quot;) plot(DVDx,method=&quot;graph&quot;) 3.2.4 By Eclat: Equivalence Class Clustering and bottom-up Lattice Traversal. 3.2.4.1 get itemset eclat(data, parameter = NULL, control = NULL, ...): Mine frequent itemsets with the Eclat algorithm. data: transactions object parameter: named list or ECparameter object default support 0.1 maxlen 5 control: named list or ECcontrol object itemsets &lt;- eclat(newdata1, parameter = list(supp = 0.1, maxlen=5)) itemsets ## set of 53 itemsets inspectDT(itemsets) supportingTransactions(x, transactions): 查找關聯(associations)物件中每個項目集(itemset)支持(support)的對象，回傳tidLists物件 x: a set of associations (itemsets, rules, etc.) transactions: an object of class transactions used to mine the associations in x. supportingTransactions(itemsets, newdata1) ## tidLists in sparse format with ## 53 items/itemsets (rows) and ## 10 transactions (columns) e.g. 第一筆表示{Harry Potter1}子集有出現在第1和7筆交易中 as(supportingTransactions(itemsets, newdata1), &#39;list&#39;)[c(48:50)] ## $`{Harry Potter1}` ## [1] &quot;1&quot; &quot;7&quot; ## ## $`{LOTR1}` ## [1] &quot;1&quot; &quot;3&quot; ## ## $`{LOTR2}` ## [1] &quot;1&quot; &quot;3&quot; inspect(newdata1, linebreak=F) ## items transactionID ## [1] {Green Mile, Harry Potter1, LOTR1, LOTR2, Sixth Sense} 1 ## [2] {Braveheart, Gladiator, Patriot} 2 ## [3] {LOTR1, LOTR2} 3 ## [4] {Gladiator, Patriot, Sixth Sense} 4 ## [5] {Gladiator, Patriot, Sixth Sense} 5 ## [6] {Gladiator, Patriot, Sixth Sense} 6 ## [7] {Harry Potter1, Harry Potter2} 7 ## [8] {Gladiator, Patriot} 8 ## [9] {Gladiator, Patriot, Sixth Sense} 9 ## [10] {Gladiator, Green Mile, LOTR, Sixth Sense} 10 3.2.4.2 get rules from itemset ruleInduction(x, transactions = NULL, confidence = 0.8, method = c(\"ptree\", \"apriori\"),...): x: the set of itemsets from which rules will be induced. transactions: the transactions used to mine the itemsets. Can be omitted for method “ptree”, if x contains a (complete set) of frequent itemsets together with their support counts. confidence: minimum confidence threshold for the rules. ## Create rules from the frequent itemsets rules &lt;- ruleInduction(itemsets, confidence = .1) rules ## set of 117 rules inspectDT(rules) 3.2.4.3 get frequent itemsets with Eclat ## Mine frequent itemsets with Eclat. fsets &lt;- eclat(newdata1, parameter = list(supp = 0.2)) fsets.top5 &lt;- sort(fsets)[1:5] inspect(fsets.top5) ## items support count ## [1] {Gladiator} 0.7 7 ## [2] {Gladiator, Patriot} 0.6 6 ## [3] {Sixth Sense} 0.6 6 ## [4] {Patriot} 0.6 6 ## [5] {Gladiator, Sixth Sense} 0.5 5 as(items(fsets.top5), &quot;list&quot;) ## [[1]] ## [1] &quot;Gladiator&quot; ## ## [[2]] ## [1] &quot;Gladiator&quot; &quot;Patriot&quot; ## ## [[3]] ## [1] &quot;Sixth Sense&quot; ## ## [[4]] ## [1] &quot;Patriot&quot; ## ## [[5]] ## [1] &quot;Gladiator&quot; &quot;Sixth Sense&quot; as(items(fsets.top5), &quot;matrix&quot;) |&gt; as.data.frame() |&gt; knitr::kable(row.names = T) Braveheart Gladiator Green Mile Harry Potter1 Harry Potter2 LOTR LOTR1 LOTR2 Patriot Sixth Sense 1 FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE 2 FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE 3 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE 4 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE 5 FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE 3.3 Example: Income 3.3.1 explore data IncomeESL0=read.csv(&quot;data/IncomeESL.csv&quot;) dim(IncomeESL0) ## [1] 8993 14 IncomeESL=IncomeESL0[complete.cases(IncomeESL0),] # remove missing obs. dim(IncomeESL) ## [1] 6876 14 colnames(IncomeESL) |&gt; matrix(ncol=7) |&gt; knitr::kable() income marital.status education years.in.bay.area number.in.household householder.status ethnic.classification sex age occupation dual.incomes number.of.children type.of.home language.in.home Income &lt;- as(IncomeESL, &quot;transactions&quot;) summ &lt;- summary(Income) Income@itemInfo |&gt; DT::datatable(filter = &#39;top&#39;) @itemSummary: 最頻繁項目 @Dim: [rows (elements/itemsets/transactions), columns (items)] @density: 稀疏矩陣的密度 summ@itemSummary # most frequent items ## language.in.home=english ethnic.classification=white ## 6277 4605 ## years.in.bay.area=&gt;10 number.of.children=0 ## 4446 4276 ## dual.incomes=not married (Other) ## 4114 72546 summ@Dim ## [1] 6876 84 summ@density ## [1] 0.1666667 用於交易物件的函數 summary(transactions) toLongFormat(transactions): 轉成兩欄的data.frame，分別為ID和項目，可用cols重新命名 items(transactions): 轉為itemMatrix transactionInfo(transactions) &lt;- value dimnames(transactions) &lt;- value # summary(Income) toLongFormat(Income) |&gt; head(5) ## TID item ## 1 1 income=75+ ## 2 1 sex=male ## 3 1 marital.status=married ## 4 1 age=45-54 ## 5 1 education=college graduate items(Income) ## itemMatrix in sparse format with ## 6876 rows (elements/transactions) and ## 84 columns (items) # transactionInfo(Income) |&gt; head(3) # dimnames(Income) itemFrequency(x, type = c(\"relative\", \"absolute\")): 取得所有單個項目的次數/支持度 sort(itemFrequency(Income), decreasing = T) |&gt; head(5) ## language.in.home=english ethnic.classification=white ## 0.9128854 0.6697208 ## years.in.bay.area=&gt;10 number.of.children=0 ## 0.6465969 0.6218732 ## dual.incomes=not married ## 0.5983130 itemFrequencyPlot(Income, support=0.5, cex.names = 1.2, main=&#39;only support &gt; 0.5&#39;) itemFrequencyPlot(Income, topN=10, horiz=T, main=&#39;top 10 highest freq.&#39;) las: 標籤的方向，垂直和水平指x, y軸標籤之間 0: (預設)垂直放置 1: 水平放置 2: 垂直放置，但文字旋轉90度 3: 水平放置，但文字旋轉90度 par(mar=c(3,13,1,1)) barplot(sort(itemFrequency(Income), decreasing=TRUE), horiz =T, las=2) 3.3.2 get rules with apriori rules &lt;- apriori(Income, parameter = list(support = 0.2, confidence = 0.6)) summ &lt;- summary(rules) @quality: 支持度、信賴度、覆蓋率(coverage)、增益度、次數的基本統計量 @lengths: LHS + RHS的總項目數分布 summ@quality |&gt; knitr::kable() support confidence coverage lift count Min. :0.2001 Min. :0.6004 Min. :0.2001 Min. :0.9215 Min. :1376 1st Qu.:0.2247 1st Qu.:0.6746 1st Qu.:0.2761 1st Qu.:1.0189 1st Qu.:1545 Median :0.2469 Median :0.7501 Median :0.3380 Median :1.0816 Median :1698 Mean :0.2800 Mean :0.7875 Mean :0.3663 Mean :1.1999 Mean :1925 3rd Qu.:0.2922 3rd Qu.:0.9240 3rd Qu.:0.4007 3rd Qu.:1.1930 3rd Qu.:2009 Max. :0.9129 Max. :1.0000 Max. :1.0000 Max. :2.4343 Max. :6277 summ@lengths ## sizes ## 1 2 3 4 ## 4 73 176 83 inspectDT(rules) |&gt; DT::formatRound(columns=&#39;count&#39;, digits=0) plot(rules, measure=c(&quot;confidence&quot;,&quot;lift&quot;), shading=&quot;support&quot;)+ geom_point(size=3) plot(rules, method=&quot;grouped&quot;, control=list(k=10)) 3.3.2.1 Check only RHS is “householder.status=own” rulesOwned &lt;- subset(rules,subset= rhs %in% &quot;householder.status=own&quot; &amp; lift&gt;1) DATAFRAME(head(sort(rulesOwned,by=&quot;support&quot;),n=5)) |&gt; format(digits=2) |&gt; knitr::kable() LHS RHS support confidence coverage lift count 28 {marital.status=married} {householder.status=own} 0.26 0.68 0.39 1.8 1798 116 {marital.status=married,language.in.home=english} {householder.status=own} 0.25 0.70 0.36 1.9 1700 107 {marital.status=married,type.of.home=house} {householder.status=own} 0.23 0.83 0.28 2.2 1602 260 {marital.status=married,type.of.home=house,language.in.home=english} {householder.status=own} 0.22 0.84 0.26 2.2 1518 113 {marital.status=married,ethnic.classification=white} {householder.status=own} 0.20 0.74 0.28 2.0 1409 plot(rulesOwned,method=&quot;grouped&quot;) 3.4 [處理多餘規則] Example: Groceries 3.4.1 基本處理 觀察資料 library(arules) library(arulesViz) data(Groceries) itemFrequencyPlot(Groceries,topN=20) 取得規則 rules1 &lt;- apriori(Groceries, parameter = list(supp = 0.001, conf = 0.8)) 顯示前5個規則，小數點取到第2位 options(digits=2) DATAFRAME(rules1[1:5], separate=F) |&gt; knitr::kable() rule support confidence coverage lift count {liquor,red/blush wine} =&gt; {bottled beer} 0 0.90 0 11.2 19 {curd,cereals} =&gt; {whole milk} 0 0.91 0 3.6 10 {yogurt,cereals} =&gt; {whole milk} 0 0.81 0 3.2 17 {butter,jam} =&gt; {whole milk} 0 0.83 0 3.3 10 {soups,bottled beer} =&gt; {whole milk} 0 0.92 0 3.6 11 根據信賴度排序，由高到低 rules1 &lt;- sort(rules1, by=&quot;confidence&quot;, decreasing=TRUE) DATAFRAME(rules[1:5], separate=F) |&gt; knitr::kable(caption = &quot;Rules with top 5 highest confidence&quot;) Table 3.1: Rules with top 5 highest confidence rule support confidence coverage lift count {} =&gt; {number.of.children=0} 0.62 0.62 1.00 1.0 4276 {} =&gt; {years.in.bay.area=&gt;10} 0.65 0.65 1.00 1.0 4446 {} =&gt; {ethnic.classification=white} 0.67 0.67 1.00 1.0 4605 {} =&gt; {language.in.home=english} 0.91 0.91 1.00 1.0 6277 {age=18-24} =&gt; {dual.incomes=not married} 0.20 0.86 0.23 1.4 1387 3.4.2 處理冗贅的規則 較長的規則 從表3.1可以看到後面幾個規則比較長，我們可以設定maxlen來限制規則的長度 rules2 &lt;- apriori(Groceries, parameter = list(supp = 0.001, conf = 0.8,maxlen=3)) rules2 &lt;- sort(rules2, by=&quot;confidence&quot;, decreasing=TRUE) DATAFRAME(rules[1:5], separate=F) |&gt; knitr::kable() rule support confidence coverage lift count {} =&gt; {number.of.children=0} 0.62 0.62 1.00 1.0 4276 {} =&gt; {years.in.bay.area=&gt;10} 0.65 0.65 1.00 1.0 4446 {} =&gt; {ethnic.classification=white} 0.67 0.67 1.00 1.0 4605 {} =&gt; {language.in.home=english} 0.91 0.91 1.00 1.0 6277 {age=18-24} =&gt; {dual.incomes=not married} 0.20 0.86 0.23 1.4 1387 多餘的規則 有時候規則會重複，下面顯示的兩個規則差別在第2個規則多了”tropical fruit”，但規則2的 lift \\(\\leq\\) 規則1的 lift，表示加入”tropical fruit”對預測的提升沒有幫助，則這條規則是多餘的 rules3 &lt;- sort(rules1, by=&quot;lift&quot;) options(digits=2) subset(rules3, subset=rhs %in% &quot;whole milk&quot; &amp; lift&gt;3.7)[c(13,20)] |&gt; inspect(linebreak = T) ## lhs rhs support confidence coverage lift count ## [1] {root vegetables, ## other vegetables, ## yogurt, ## oil} =&gt; {whole milk} 0.0014 1 0.0014 3.9 14 ## [2] {tropical fruit, ## root vegetables, ## other vegetables, ## yogurt, ## oil} =&gt; {whole milk} 0.0010 1 0.0010 3.9 10 可以用is.subset(x,y)判斷，x的項目如果是y項目的子集，就會回串TRUE。為了方便呈現整體因此先隱藏行列的名稱 主對角線皆為T，因為自己是自己的子集。 從下面的結果可以看到，在(12,20)和(13,20)處是T，表示第12, 13列的項目是第20行的子集。 詳細內容見表3.2 rules4 &lt;- subset(rules3, subset=rhs %in% &quot;whole milk&quot; &amp; lift&gt;3.7) subset.matrix &lt;- is.subset(rules4, rules4) subset.matrix1 &lt;- as.matrix(subset.matrix) subset.matrix |&gt; unname() ## 20 x 20 sparse Matrix of class &quot;ngCMatrix&quot; ## ## [1,] | . . . . . . . . . . . . . . . . . . . ## [2,] . | . . . . . . . . . . . . . . . . . . ## [3,] . . | . . . . . . . . . . . . . . . . . ## [4,] . . . | . . . . . . . . . . . . . . . . ## [5,] . . . . | . . . . . . . . . . . . . . . ## [6,] . . . . . | . . . . . . . . . . . . . . ## [7,] . . . . . . | . . . . . . . . . . . . . ## [8,] . . . . . . . | . . . . . . . . . . . . ## [9,] . . . . . . . . | . . . . . . . . . . . ## [10,] . . . . . . . . . | . . . . . . . . . . ## [11,] . . . . . . . . . . | . . . . . . . . . ## [12,] . . . . . . . . . . . | . . . . . . . | ## [13,] . . . . . . . . . . . . | . . . . . . | ## [14,] . . . . . . . . . . . . . | . . . . . . ## [15,] . . . . . . . . . . . . . . | . . . . . ## [16,] . . . . . . . . . . . . . . . | . . . . ## [17,] . . . . . . . . . . . . . . . . | . . . ## [18,] . . . . . . . . . . . . . . . . . | . . ## [19,] . . . . . . . . . . . . . . . . . . | . ## [20,] . . . . . . . . . . . . . . . . . . . | target &lt;- subset.matrix1[c(12,13),20] |&gt; matrix(nrow=2) colnames(target) &lt;- colnames(subset.matrix1)[20] rownames(target) &lt;- rownames(subset.matrix1)[c(12,13)] target |&gt; knitr::kable(emphasize.rownames=F, caption=&quot;Redudant rules&quot;) Table 3.2: Redudant rules {tropical fruit,root vegetables,other vegetables,whole milk,yogurt,oil} {tropical fruit,root vegetables,whole milk,yogurt,oil} TRUE {root vegetables,other vegetables,whole milk,yogurt,oil} TRUE 移除下三角的矩陣，包含對角線 subset.matrix1[lower.tri(subset.matrix1, diag=T)] &lt;- NA subset.matrix1[1:5,1:5] |&gt; unname() ## [,1] [,2] [,3] [,4] [,5] ## [1,] NA FALSE FALSE FALSE FALSE ## [2,] NA NA FALSE FALSE FALSE ## [3,] NA NA NA FALSE FALSE ## [4,] NA NA NA NA FALSE ## [5,] NA NA NA NA NA 計算每行中TRUE的個數，如果有一個(含)以上，表示它是多餘的規則 從表3.2可以看到{tropical fruit,root vegetables,other vegetables,whole milk,yogurt,oil}，有兩個TRUE，表示它有其它的子集，因此它是多餘的 redundant &lt;- colSums(subset.matrix1, na.rm=T) &gt;= 1 sum(redundant) ## [1] 1 我們發現了1條多餘的規則，移除它並檢查是否有成功 rules.pruned &lt;- rules4[!redundant] summary(rules4)@lengths ## sizes ## 3 4 5 6 ## 2 9 8 1 summary(rules.pruned)@lengths ## sizes ## 3 4 5 ## 2 9 8 "],["self-learning-1.html", "Sec 4 Self-learning 4.1 check details of Income data set 4.2 [compare different grouping] regroup income into 2 levels 4.3 [compare different algorithm] mine rule of AdultUCI data set 4.4 就診紀錄 4.5 早餐店購買紀錄 4.6 Groceries data", " Sec 4 Self-learning library(arules) library(arulesViz) library(ggplot2) library(cowplot) library(kableExtra) library(huxtable) 表格輸出設定，因為gitbook會把寬的表格給cut掉 print.rule.table &lt;- function(rule, sub=F, ..., sort=T, sortBy=&#39;lift&#39;, all=F, headN=5, select=F, selected, digits=2, scroll=F, kable=T){ if(sub) out &lt;- subset(rule, ...) else out &lt;- rule if(length(out)==0) message(&#39;no rules in this condition.&#39;) else { if(sort) out &lt;- sort(out, by=sortBy) else out &lt;- rule if(all) out &lt;- out else out &lt;- head(out, headN) if(select){ if (missing(selected)) warning(&quot;selected column should be specified.&quot;) else out &lt;- DATAFRAME(out)[, c(&quot;LHS&quot;, &quot;RHS&quot;, selected)] } else out &lt;- DATAFRAME(out) if(kable){ out &lt;- format(out, digits=digits) |&gt; knitr::kable() if(scroll){ out &lt;- out |&gt; kable_styling() |&gt; scroll_box(width = &quot;100%&quot;, box_css = &quot;border: 0px;&quot;) } } else out &lt;- format(out, digits=digits) return(out) } } 比較不同的規則，列出相同條件下，不同方法取出的規則 compare.rule &lt;- function(rule1, rule2, ...){ row1 &lt;- subset(rule1@quality, ...) |&gt; rownames() row2 &lt;- subset(rule2@quality, ...) |&gt; rownames() list(rule1[row1], rule2[row2]) } 映射滿足條件的欄位 mapping &lt;- function(df, rangeVec, tagetCol){ df |&gt; as_hux() |&gt; map_background_color(everywhere, tagetCol, by_ranges(rangeVec, c(NA, &#39;#FEF9E7&#39;, NA))) |&gt; map_text_color(everywhere, tagetCol, by_ranges(rangeVec, c(NA, &#39;#212F3C&#39;, NA))) } 4.1 check details of Income data set Using package dataset “Income”, and check package PDF manual for details, and explain the code via markdown. Income: 交易物件(transactions object)，包含6876筆交易、50個項目(隸屬於14個變數) income, sex, marital status, age, education, occupation, years in bay area, dual incomes, number in household, number of children, householder status, type of home, ethnic classification, language in home 資料處理: 1. 移除包含遺失值的資料 2. 以中位數將順序尺度(ordinal)變數分為兩類 data(&quot;IncomeESL&quot;) cum.freq &lt;- function(df){ var.ord.fac &lt;- sapply(df, is.ordered) sapply(df[,var.ord.fac], table) } cum.freq(IncomeESL) ## $income ## ## [0,10) [10,15) [15,20) [20,25) [25,30) [30,40) [40,50) [50,75) 75+ ## 1745 775 667 813 722 1110 969 1308 884 ## ## $age ## ## 14-17 18-24 25-34 35-44 45-54 55-64 65+ ## 878 2129 2249 1615 922 640 560 ## ## $education ## ## grade &lt;9 grades 9-11 high school graduate ## 264 1046 2041 ## college (1-3 years) college graduate graduate study ## 3066 1524 966 ## ## $`years in bay area` ## ## &lt;1 1-3 4-6 7-10 &gt;10 ## 270 1042 686 900 5182 ## ## $`number in household` ## ## 1 2 3 4 5 6 7 8 9+ ## 1620 2664 1670 1526 686 239 105 56 52 ## ## $`number of children` ## ## 0 1 2 3 4 5 6 7 8 9+ ## 5724 1506 1148 412 117 46 27 8 2 3 ## remove incomplete cases IncomeESL &lt;- IncomeESL[complete.cases(IncomeESL), ] ## preparing the data set IncomeESL[[&quot;income&quot;]] &lt;- factor((as.numeric(IncomeESL[[&quot;income&quot;]]) &gt; 6) +1, levels = 1 : 2 , labels = c(&quot;$0-$40,000&quot;, &quot;$40,000+&quot;)) IncomeESL[[&quot;age&quot;]] &lt;- factor((as.numeric(IncomeESL[[&quot;age&quot;]]) &gt; 3) +1, levels = 1 : 2 , labels = c(&quot;14-34&quot;, &quot;35+&quot;)) IncomeESL[[&quot;education&quot;]] &lt;- factor((as.numeric(IncomeESL[[&quot;education&quot;]]) &gt; 4) +1, levels = 1 : 2 , labels = c(&quot;no college graduate&quot;, &quot;college graduate&quot;)) IncomeESL[[&quot;years in bay area&quot;]] &lt;- factor( (as.numeric(IncomeESL[[&quot;years in bay area&quot;]]) &gt; 4) +1, levels = 1 : 2 , labels = c(&quot;1-9&quot;, &quot;10+&quot;)) IncomeESL[[&quot;number in household&quot;]] &lt;- factor( (as.numeric(IncomeESL[[&quot;number in household&quot;]]) &gt; 3) +1, levels = 1 : 2 , labels = c(&quot;1&quot;, &quot;2+&quot;)) IncomeESL[[&quot;number of children&quot;]] &lt;- factor( (as.numeric(IncomeESL[[&quot;number of children&quot;]]) &gt; 1) +0, levels = 0 : 1 , labels = c(&quot;0&quot;, &quot;1+&quot;)) ## creating transactions Income &lt;- transactions(IncomeESL) data(&quot;Income&quot;) Income@itemInfo |&gt; DT::datatable(filter = &#39;top&#39;) 4.2 [compare different grouping] regroup income into 2 levels Use “IncomeESL”, merge income into two levels c(“$40-”,“$40+”). Compare your results. 重新切分 \\(\\to\\) 類似商品層級 e.g. 衣服 \\(\\to\\) 長袖、短袖、外套、… 4.2.1 explore data 對 income 變數進行重新分組，該變數共9組，各組個數如下 data(&quot;IncomeESL&quot;) levels(IncomeESL[[&quot;income&quot;]]) ## [1] &quot;[0,10)&quot; &quot;[10,15)&quot; &quot;[15,20)&quot; &quot;[20,25)&quot; &quot;[25,30)&quot; &quot;[30,40)&quot; &quot;[40,50)&quot; ## [8] &quot;[50,75)&quot; &quot;75+&quot; table(IncomeESL[[&quot;income&quot;]]) ## ## [0,10) [10,15) [15,20) [20,25) [25,30) [30,40) [40,50) [50,75) 75+ ## 1745 775 667 813 722 1110 969 1308 884 比較把 income 重新分為2組($40-, $40+)和3組($20-, $20-$40, $40+)對探勘關聯規則有甚麼影響 df.2g &lt;- df.3g &lt;- IncomeESL df.2g[[&quot;income&quot;]] &lt;- factor((as.numeric(df.2g[[&quot;income&quot;]]) &gt; 6) +1, levels = 1:2 , labels = c(&quot;$40-&quot;, &quot;$40+&quot;)) # table(df.2g$income) Income.2g &lt;- as(df.2g, &#39;transactions&#39;) df.3g[[&quot;income&quot;]] &lt;- factor( ifelse(as.numeric(df.3g[[&quot;income&quot;]]) &lt; 4, 1, ifelse(as.numeric(df.3g[[&quot;income&quot;]]) &lt; 7, 2, 3)), levels = 1:3 , labels = c(&quot;$20-&quot;, &quot;$20-$40&quot;, &quot;$40+&quot;)) # table(df.3g$income) Income.3g &lt;- as(df.3g, &#39;transactions&#39;) 下面兩張圖分別是 income 分成2組和3組時，次數最多的10個項目，主要差別用紅線標記 分成2組時，income = $40- 為第2高 分成3組時，income 沒有在前10名內 par(mfrow=c(2,1), mar=c(4,3,1,1)) itemFrequencyPlot(Income.2g, topN=10, horiz=T, main=&#39;top 10 highest freq. with 2 group income&#39;) abline(h=1.9, col=&#39;red&#39;) itemFrequencyPlot(Income.3g, topN=10, horiz=T, main=&#39;top 10 highest freq. with 3 group income&#39;) abline(h=10.3, col=&#39;red&#39;) 4.2.2 explore rule 設定最小支持度為0.4、信賴度0.5: 保留常見的規則 rule.2g &lt;- apriori(Income.2g, parameter = list(support = 0.4, confidence = 0.5)) rule.3g &lt;- apriori(Income.3g, parameter = list(support = 0.4, confidence = 0.5)) 不同項目集個數的規則數 (RHS+LHS) 2組: 38個規則 3組: 28個規則 summary(rule.2g)@lengths ## sizes ## 1 2 3 ## 8 24 6 summary(rule.3g)@lengths ## sizes ## 1 2 3 ## 7 18 3 以增益度(lift)排序，顯示前5高的規則 兩種分組的前2個規則相同 print.rule.table(rule.2g, scroll = T) print.rule.table(rule.3g, scroll = T) LHS RHS support confidence coverage lift count 9 {marital status=single} {dual incomes=not married} 0.41 1.00 0.41 1.7 3654 10 {dual incomes=not married} {marital status=single} 0.41 0.67 0.60 1.7 3654 35 {income=$40-,language in home=english} {dual incomes=not married} 0.41 0.74 0.55 1.2 3670 20 {income=$40-} {dual incomes=not married} 0.48 0.73 0.65 1.2 4282 19 {dual incomes=not married} {income=$40-} 0.48 0.79 0.60 1.2 4282 LHS RHS support confidence coverage lift count 8 {marital status=single} {dual incomes=not married} 0.41 1.00 0.41 1.7 3654 9 {dual incomes=not married} {marital status=single} 0.41 0.67 0.60 1.7 3654 27 {number of children=0,language in home=english} {ethnic classification=white} 0.42 0.75 0.56 1.2 3787 26 {number of children=0,ethnic classification=white} {language in home=english} 0.42 0.95 0.45 1.1 3787 24 {ethnic classification=white} {language in home=english} 0.61 0.95 0.65 1.1 5495 分成2組(上)和3組(下)時的分布圖 多出來的10個規則大概分布在信賴度 = 0.65 和 0.75 處 par(mfrow=c(2,1), mar=c(3,3,1,1)) p1 &lt;- plot(rule.2g, measure=c(&quot;confidence&quot;,&quot;lift&quot;), shading=&quot;support&quot;)+ geom_point(size=3) + geom_hline(yintercept = 1, col=&#39;blue&#39;) p2 &lt;- plot(rule.3g, measure=c(&quot;confidence&quot;,&quot;lift&quot;), shading=&quot;support&quot;)+ geom_point(size=3) + geom_hline(yintercept = 1, col=&#39;blue&#39;) plot_grid(p1, p2, nrow=2) 檢視信賴度Range=(0.7, 0.8)且增益度Range=(1.1, 1.3)處的規則(目測約5個) 分成2組多出的規則為前4筆 results &lt;- compare.rule(rule.2g, rule.3g, confidence&gt;0.7 &amp; confidence&lt;0.8 &amp; lift&gt;1.1 &amp; lift&lt;1.3) print.rule.table(results[[1]], all=T, scroll=T) print.rule.table(results[[2]], all=T, scroll=T) LHS RHS support confidence coverage lift count 35 {income=$40-,language in home=english} {dual incomes=not married} 0.41 0.74 0.55 1.2 3670 20 {income=$40-} {dual incomes=not married} 0.48 0.73 0.65 1.2 4282 19 {dual incomes=not married} {income=$40-} 0.48 0.79 0.60 1.2 4282 34 {dual incomes=not married,language in home=english} {income=$40-} 0.41 0.78 0.52 1.2 3670 37 {number of children=0,language in home=english} {ethnic classification=white} 0.42 0.75 0.56 1.2 3787 LHS RHS support confidence coverage lift count 27 {number of children=0,language in home=english} {ethnic classification=white} 0.42 0.75 0.56 1.2 3787 4.3 [compare different algorithm] mine rule of AdultUCI data set 因為fim4r載不了，所以只比較apriori和eclat Importing built-in data “AdultUCI” and mining rules 4.3.1 details of data AdultUCI資料包含成人資料庫(人口普查收入資料庫)的問卷資料，包含 48,842個觀察值 15個變數: age, workclass, fnlwgt, education, education-num, marital-status, occupation, relationship, race, sex, capital-gain, capital-loss, hours-per-week, native-country, income Adult資料為交易物件，包含 48,842筆交易 115個項目 資料處理如下 移除連續特徵fnlwgt(final weight) 移除education-num，因為它只是education的數值表示 年齡(age)分為Young(0-25), Middle-aged(26-45), Senior(46-65)和Old(66+) 每周工作時間(hours-per-week)分為Part-time(0-25), Full-time(25-40), Over-time(40-60)和Too-much(60+) 資本收益和損失(capital-gain / capital-loss)分為None(0), Low(0 &lt; 大於0的數值之中位數 &lt; 最大值) and High( \\(\\geq\\) 最大值) capital-gain: 大於0的數值之中位數=7298，分組為(-Inf,0], (0,7.3e+03], (7.3e+03, Inf] capital-loss: 大於0的數值之中位數=1887，分組為(-Inf,0], (0,1.89e+03], (1.89e+03, Inf] data(&quot;AdultUCI&quot;) ## remove attributes AdultUCI[[&quot;fnlwgt&quot;]] &lt;- NULL AdultUCI[[&quot;education-num&quot;]] &lt;- NULL ## map metric attributes # min=17, max=90 AdultUCI[[&quot;age&quot;]] &lt;- ordered(cut(AdultUCI[[&quot;age&quot;]], c(15, 25, 45, 65, 100)), labels = c(&quot;Young&quot;, &quot;Middle-aged&quot;, &quot;Senior&quot;, &quot;Old&quot;)) # min=1, max=99 AdultUCI[[&quot;hours-per-week&quot;]] &lt;- ordered(cut(AdultUCI[[&quot;hours-per-week&quot;]], c(0,25,40,60,168)), labels = c(&quot;Part-time&quot;, &quot;Full-time&quot;, &quot;Over-time&quot;, &quot;Workaholic&quot;)) # min=0, max=99999 AdultUCI[[&quot;capital-gain&quot;]] &lt;- ordered(cut(AdultUCI[[&quot;capital-gain&quot;]], c(-Inf,0,median(AdultUCI[[&quot;capital-gain&quot;]][AdultUCI[[&quot;capital-gain&quot;]] &gt; 0]), Inf)), labels = c(&quot;None&quot;, &quot;Low&quot;, &quot;High&quot;)) AdultUCI[[&quot;capital-loss&quot;]] &lt;- ordered(cut(AdultUCI[[&quot;capital-loss&quot;]], c(-Inf,0, median(AdultUCI[[&quot;capital-loss&quot;]][AdultUCI[[&quot;capital-loss&quot;]] &gt; 0]), Inf)), labels = c(&quot;None&quot;, &quot;Low&quot;, &quot;High&quot;)) ## create transactions Adult &lt;- transactions(AdultUCI) cut(x, breaks, ...): 把x切分成不同的區間 tx0 &lt;- c(9, 4, 6, 5, 3, 10, 5, 3, 5) x &lt;- rep(0:8, tx0) table(cx &lt;- cut(x, breaks = 2*(0:4))) ## ## (0,2] (2,4] (4,6] (6,8] ## 10 8 15 8 table(cxl &lt;- cut(x, breaks = 2*(0:4), right = FALSE)) ## ## [0,2) [2,4) [4,6) [6,8) ## 13 11 13 8 4.3.2 explore data 出現最多次的是資本損失和收益為無，接著是出生地為美國，種族為白人(因為是美國的人口普查) data(&quot;Adult&quot;) itemFrequencyPlot(Adult, topN=10, horiz=T, main=&#39;top 10 highest freq.&#39;) 4.3.3 explore rule 設定最小支持度為0.4、信賴度0.5: 保留常見的規則 calc.time &lt;- function(code){ set.seed(123) t1 &lt;- Sys.time() code t2 &lt;- Sys.time() return(t2-t1) } # Apriori # rule.apriori &lt;- apriori(Adult, parameter = list(support = 0.4, confidence = 0.5)) t1 &lt;- calc.time(rule.apriori &lt;- apriori(Adult, parameter = list(support = 0.4, confidence = 0.5))) # Eclat itemset.eclat &lt;- eclat(Adult, parameter = list(supp = 0.4, maxlen=5)) # rule.eclat &lt;- ruleInduction(itemset.eclat, confidence = 0.5) t2 &lt;- calc.time(rule.eclat &lt;- ruleInduction(itemset.eclat, confidence = 0.5)) # FP-Growth # rule.fpgrowth &lt;- fim4r(Adult, method = &quot;fpgrowth&quot;, target = &quot;rules&quot;, supp = .7, conf = .8) 兩種演算法的運算時間 Algorithm Time (sec) apriori 0.06773 eclat 0.02415 apriori 和 eclat 的分布圖 無明顯差異 相差 9 個規則 p1 &lt;- plot(rule.apriori, measure=c(&quot;confidence&quot;,&quot;lift&quot;), shading=&quot;support&quot;)+ geom_point(size=3) + scale_colour_gradient(low=&#39;steelblue1&#39;, high=&#39;firebrick1&#39;, limits=c(0.4,1)) p2 &lt;- plot(rule.eclat, measure=c(&quot;confidence&quot;,&quot;lift&quot;), shading=&quot;support&quot;)+ geom_point(size=3) + scale_colour_gradient(low=&#39;steelblue1&#39;, high=&#39;firebrick1&#39;, limits=c(0.4,1)) plot_grid(p1, p2, nrow=2) 以支持度 (support) 排序，顯示前5高的規則 eclat 的 LHS 沒有空集合 eclat 沒有 count 和 coverage print.rule.table(rule.apriori, sortBy = &#39;support&#39;) print.rule.table(rule.eclat, sortBy = &#39;support&#39;) LHS RHS support confidence coverage lift count 9 {} {capital-loss=None} 0.95 0.95 1.00 1 46560 8 {} {capital-gain=None} 0.92 0.92 1.00 1 44807 7 {} {native-country=United-States} 0.90 0.90 1.00 1 43832 72 {capital-gain=None} {capital-loss=None} 0.87 0.95 0.92 1 42525 73 {capital-loss=None} {capital-gain=None} 0.87 0.91 0.95 1 42525 LHS RHS support confidence lift 246 {capital-loss=None} {capital-gain=None} 0.87 0.91 1 247 {capital-gain=None} {capital-loss=None} 0.87 0.95 1 242 {native-country=United-States} {capital-loss=None} 0.85 0.95 1 243 {capital-loss=None} {native-country=United-States} 0.85 0.90 1 244 {native-country=United-States} {capital-gain=None} 0.82 0.92 1 以信賴度 (confidence) 排序，顯示前5高的規則 兩種演算法的規則相同 print.rule.table(rule.apriori, sortBy = &#39;confidence&#39;, scroll = T) print.rule.table(rule.eclat, sortBy = &#39;confidence&#39;) LHS RHS support confidence coverage lift count 12 {relationship=Husband} {sex=Male} 0.4 1.00 0.40 1.5 19715 74 {marital-status=Married-civ-spouse,relationship=Husband} {sex=Male} 0.4 1.00 0.40 1.5 19703 10 {relationship=Husband} {marital-status=Married-civ-spouse} 0.4 1.00 0.40 2.2 19704 75 {relationship=Husband,sex=Male} {marital-status=Married-civ-spouse} 0.4 1.00 0.40 2.2 19703 76 {marital-status=Married-civ-spouse,sex=Male} {relationship=Husband} 0.4 0.99 0.41 2.5 19703 LHS RHS support confidence lift 5 {relationship=Husband} {sex=Male} 0.4 1.00 1.5 3 {marital-status=Married-civ-spouse,relationship=Husband} {sex=Male} 0.4 1.00 1.5 6 {relationship=Husband} {marital-status=Married-civ-spouse} 0.4 1.00 2.2 1 {relationship=Husband,sex=Male} {marital-status=Married-civ-spouse} 0.4 1.00 2.2 2 {marital-status=Married-civ-spouse,sex=Male} {relationship=Husband} 0.4 0.99 2.5 以增益度 (lift) 排序，顯示前5高的規則 兩種演算法的規則相同 print.rule.table(rule.apriori, sortBy = &#39;lift&#39;, scroll = T) print.rule.table(rule.eclat, sortBy = &#39;lift&#39;) LHS RHS support confidence coverage lift count 76 {marital-status=Married-civ-spouse,sex=Male} {relationship=Husband} 0.4 0.99 0.41 2.5 19703 11 {marital-status=Married-civ-spouse} {relationship=Husband} 0.4 0.88 0.46 2.2 19704 10 {relationship=Husband} {marital-status=Married-civ-spouse} 0.4 1.00 0.40 2.2 19704 75 {relationship=Husband,sex=Male} {marital-status=Married-civ-spouse} 0.4 1.00 0.40 2.2 19703 13 {sex=Male} {relationship=Husband} 0.4 0.60 0.67 1.5 19715 LHS RHS support confidence lift 2 {marital-status=Married-civ-spouse,sex=Male} {relationship=Husband} 0.4 0.99 2.5 6 {relationship=Husband} {marital-status=Married-civ-spouse} 0.4 1.00 2.2 7 {marital-status=Married-civ-spouse} {relationship=Husband} 0.4 0.88 2.2 1 {relationship=Husband,sex=Male} {marital-status=Married-civ-spouse} 0.4 1.00 2.2 4 {sex=Male} {relationship=Husband} 0.4 0.60 1.5 4.3.4 conclusion apriori 比 eclat 多出來的 9 個規則為 LHS 是空集合的規則 rule.apriori[size(lhs(rule.apriori))==0] |&gt; print.rule.table(all=T) LHS RHS support confidence coverage lift count 1 {} {age=Middle-aged} 0.51 0.51 1 1 24671 2 {} {income=small} 0.51 0.51 1 1 24720 5 {} {workclass=Private} 0.69 0.69 1 1 33906 6 {} {race=White} 0.86 0.86 1 1 41762 7 {} {native-country=United-States} 0.90 0.90 1 1 43832 8 {} {capital-gain=None} 0.92 0.92 1 1 44807 9 {} {capital-loss=None} 0.95 0.95 1 1 46560 3 {} {hours-per-week=Full-time} 0.59 0.59 1 1 28577 4 {} {sex=Male} 0.67 0.67 1 1 32650 相同條件下 eclat 的運算速度較快 4.4 就診紀錄 下表紀錄了24位患者近兩年內的就診紀錄，請根據資料回答下列問題 df &lt;- read.csv(&#39;data/RA HW1-1.csv&#39;, encoding = &#39;UTF-8&#39;) colnames(df) &lt;- c(&#39;ID&#39;, paste0(&#39;sick&#39;, seq(3))) library(reshape2) df &lt;- melt(df, id=&quot;ID&quot;) df &lt;- df[,-2] df &lt;- df[!df$value==&quot;&quot;,] 共 10 種疾病 unique(df$value) ## [1] &quot;夜盲症&quot; &quot;糖尿病&quot; &quot;憂鬱症&quot; &quot;流行感冒&quot; &quot;支氣管炎&quot; &quot;貧血&quot; ## [7] &quot;骨折&quot; &quot;心肌梗塞&quot; &quot;心臟衰竭&quot; &quot;高血壓&quot; 資料框架(左) vs. 交易物件(右) 以第2個病人為例 dat=split(as.factor(df$value),as.factor(df$ID)) sick &lt;- transactions(dat) knitr::kable(list(df[df$ID==2,], DATAFRAME(sick) |&gt; head(3))) ID value 2 2 糖尿病 26 2 憂鬱症 50 2 高血壓 items transactionID {夜盲症,流行感冒} 1 {高血壓,憂鬱症,糖尿病} 2 {流行感冒,憂鬱症} 3 以支持度最低為 0.1、信賴度最低為 0.01、項目集最多 3 個，探勘規則 rule &lt;- apriori(sick, parameter = list(support=0.1, confidence=0.01, maxlen=3)) 總共 14 條，包含 1 個項目的規則 8 條，包含 2 個項目的規則 6 條 rule summary(rule)@lengths # inspectDT(rule) |&gt; DT::formatRound(columns=&#39;count&#39;, digits=0) ## set of 14 rules ## sizes ## 1 2 ## 8 6 請列出所有的 k=1 項目集，並計算其支持度。 rule[size(lhs(rule))==0] |&gt; print.rule.table(sortBy = &#39;support&#39;, all=T) |&gt; column_spec(4, color = &#39;#212F3C&#39;, background = &#39;#FEF9E7&#39;) LHS RHS support confidence coverage lift count 8 {} {流行感冒} 0.38 0.38 1 1 9 7 {} {糖尿病} 0.29 0.29 1 1 7 4 {} {支氣管炎} 0.21 0.21 1 1 5 5 {} {心臟衰竭} 0.21 0.21 1 1 5 6 {} {貧血} 0.21 0.21 1 1 5 3 {} {高血壓} 0.17 0.17 1 1 4 1 {} {骨折} 0.12 0.12 1 1 3 2 {} {夜盲症} 0.12 0.12 1 1 3 在 k=2 項目集中，請列出包含「糖尿病」之項目集與其所對應的支持度。 rule[size(lhs(rule))+size(rhs(rule))==2] |&gt; subset(subset=lhs %in% &#39;糖尿病&#39; | rhs %in% &#39;糖尿病&#39;) |&gt; print.rule.table(all=T, sort = F) |&gt; column_spec(4, color = &#39;#212F3C&#39;, background = &#39;#FEF9E7&#39;) LHS RHS support confidence coverage lift count 9 {高血壓} {糖尿病} 0.17 1.00 0.17 3.4 4 10 {糖尿病} {高血壓} 0.17 0.57 0.29 3.4 4 在 k=3 項目集中，請找出所有支持度高於0.2之規則。 支持度最低 0.1、信賴度最低 0.01 下沒有 k=3 的項目集 rule[size(lhs(rule))+size(rhs(rule))==3] ## set of 0 rules 放寬條件，找出 k=3 的項目集，沒有支持度高於 0.2 的規則 rule1 &lt;- apriori(sick, parameter = list(support=0.001, confidence=0.001)) summary(rule1)@lengths ## sizes ## 1 2 3 ## 10 28 3 rule1[size(lhs(rule1))+size(rhs(rule1))==3] |&gt; print.rule.table(all=T) |&gt; column_spec(4, color = &#39;#212F3C&#39;, background = &#39;#FEF9E7&#39;) LHS RHS support confidence coverage lift count 40 {憂鬱症,糖尿病} {高血壓} 0.042 1.00 0.042 6.0 1 39 {高血壓,憂鬱症} {糖尿病} 0.042 1.00 0.042 3.4 1 41 {高血壓,糖尿病} {憂鬱症} 0.042 0.25 0.167 3.0 1 根據(3)所找出之高頻項目集，請計算「糖尿病」對於同項目集中之另一項目之信賴度與增益。假設信賴度的門檻值為0.4，在此是否存在任何顯著之關聯規則？ k=3 項目集為 {憂鬱症, 糖尿病, 高血壓} 信賴度最低 0.4 下，{糖尿病} \\(\\to\\) {高血壓}為顯著的關聯規則 options(huxtable.bookdown = FALSE) rule1[size(lhs(rule1))+size(rhs(rule1))==2] |&gt; subset(rule1, subset = lhs %oin% &quot;糖尿病&quot; &amp; rhs %in% c(&quot;憂鬱症&quot;, &quot;高血壓&quot;)) |&gt; DATAFRAME() |&gt; mapping(c(0.4, 1.1), &quot;confidence&quot;) LHSRHSsupportconfidencecoverageliftcount {糖尿病}{憂鬱症}0.04170.1430.2921.711 {糖尿病}{高血壓}0.167&nbsp;0.5710.2923.434 請將(2)、(3)、(4)中之項目「糖尿病」依次替換為「貧血」、「高血壓」、「憂鬱症」、「夜盲症」與「流行感冒」，探討是否有任何顯著之關聯規則可被建立。 放寬條件後，k=3 項目集只有 {憂鬱症, 糖尿病, 高血壓} 信賴度最低 0.4 下，顯著的關聯規則有: {糖尿病} \\(\\to\\) {高血壓} (Q4.) {憂鬱症} \\(\\to\\) {高血壓} {憂鬱症} \\(\\to\\) {糖尿病} {高血壓} \\(\\to\\) {糖尿病}，conf = 1 表示有看高血壓的病人都有看過糖尿病 (obs. 2, 8, 20, 22) LHSRHSsupportconfidencecoverageliftcount {憂鬱症}{高血壓}0.04170.50.08333&nbsp;&nbsp;&nbsp;1 {憂鬱症}{糖尿病}0.04170.50.08331.711 LHSRHSsupportconfidencecoverageliftcount {高血壓}{憂鬱症}0.04170.250.1673&nbsp;&nbsp;&nbsp;1 {高血壓}{糖尿病}0.167&nbsp;1&nbsp;&nbsp;&nbsp;0.1673.434 \\[ conf(X\\to Y)=\\frac{P(X\\cap Y)}{P(X)}&gt;0.4\\\\ \\Rightarrow P(X\\cap Y)&gt;0.4\\cdot P(X) \\] 因此\\(sup(X\\to Y)\\)至少要\\(0.4\\cdot P(X)\\) 計算\\(sup(X)\\cdot0.4\\)，得到support至少要0.0168 RHS support lower 10 {流行感冒} 0.375 0.15 9 {糖尿病} 0.292 0.12 6 {支氣管炎} 0.208 0.08 7 {心臟衰竭} 0.208 0.08 8 {貧血} 0.208 0.08 5 {高血壓} 0.167 0.07 3 {骨折} 0.125 0.05 4 {夜盲症} 0.125 0.05 2 {憂鬱症} 0.083 0.03 1 {心肌梗塞} 0.042 0.02 找 k=2 且 support最低0.015 tmp &lt;- apriori(sick, parameter = list(support=0.015, confidence=0.001, minlen=2, maxlen=2)) print.rule.table(tmp, sub=T, confidence&gt;0.4, all=T, sortBy = &#39;confidence&#39;) |&gt; column_spec(5, color = &#39;#212F3C&#39;, background = &#39;#FEF9E7&#39;) LHS RHS support confidence coverage lift count 15 {高血壓} {糖尿病} 0.167 1.00 0.167 3.4 4 19 {支氣管炎} {流行感冒} 0.125 0.60 0.208 1.6 3 21 {心臟衰竭} {貧血} 0.125 0.60 0.208 2.9 3 22 {貧血} {心臟衰竭} 0.125 0.60 0.208 2.9 3 16 {糖尿病} {高血壓} 0.167 0.57 0.292 3.4 4 1 {憂鬱症} {高血壓} 0.042 0.50 0.083 3.0 1 3 {憂鬱症} {糖尿病} 0.042 0.50 0.083 1.7 1 5 {憂鬱症} {流行感冒} 0.042 0.50 0.083 1.3 1 4.5 早餐店購買紀錄 下表為某早餐店所統計之顧客交易紀錄，請根據資料回答下列問題 df &lt;- read.csv(&#39;data/RA HW1-2.csv&#39;, encoding = &#39;UTF-8&#39;) colnames(df) &lt;- c(&#39;ID&#39;, &quot;sex&quot;, &quot;stu&quot;, &quot;items&quot;) items &lt;- lapply(strsplit(df$items, &quot;,&quot;), function(x) gsub(&quot; &quot;, &quot;&quot;, x)) target &lt;- df[rep(1:nrow(df), times = lengths(items)), -4] for(i in 1:nrow(target)){ target$item[i] &lt;- unlist(items)[i] } 共 13 種餐點 unique(target$item) ## [1] &quot;燒餅&quot; &quot;菜包&quot; &quot;油條&quot; &quot;豆漿&quot; &quot;肉包&quot; &quot;奶茶&quot; &quot;蛋餅&quot; &quot;吐司&quot; ## [9] &quot;漢堡&quot; &quot;可樂&quot; &quot;煎餃&quot; &quot;柳橙汁&quot; &quot;鬆餅&quot; 資料框架(左) vs. 交易物件(右) 以第3個客人為例 dat=split(as.factor(c(target$item, target$sex, target$stu)), as.factor(target$ID)) breakfast &lt;- transactions(dat) knitr::kable(list(df[df$ID==3,], DATAFRAME(breakfast) |&gt; head(3))) ID sex stu items 3 3 男 是 肉包, 菜包, 奶茶 items transactionID {男,豆漿,油條,是,菜包,燒餅} 1 {男,豆漿,油條,是,菜包,燒餅} 2 {奶茶,肉包,男,是,菜包} 3 以支持度最低為 0.2、信賴度最低為 0.5、項目集最多 3 個，“性別”及”是否為學生”不會出現在右手邊，探勘規則 rule &lt;- apriori(breakfast, parameter = list(support=0.2, confidence=0.5, maxlen=3), appearance = list(default=&quot;lhs&quot;, rhs=unique(target$item))) 總共 11 條，包含 2 個項目的規則 8 條，包含 3 個項目的規則 3 條 rule summary(rule)@lengths ## set of 11 rules ## sizes ## 2 3 ## 8 3 請列出所有的 k=1 項目集，並計算其支持度。 barplot(itemFrequency(breakfast) |&gt; sort(decreasing = T), horiz = T, las=2) 在k=2 項目集中，請找出所有支持度高於0.2之規則。 rule[size(lhs(rule))+size(rhs(rule))==2] |&gt; print.rule.table(all=T, sortBy = &quot;support&quot;) |&gt; column_spec(4, color = &#39;#212F3C&#39;, background = &#39;#FEF9E7&#39;) LHS RHS support confidence coverage lift count 6 {男} {豆漿} 0.30 0.6 0.5 1.5 6 8 {是} {菜包} 0.30 0.6 0.5 1.3 6 1 {男} {燒餅} 0.25 0.5 0.5 2.0 5 2 {否} {奶茶} 0.25 0.5 0.5 1.4 5 3 {是} {油條} 0.25 0.5 0.5 1.4 5 4 {男} {油條} 0.25 0.5 0.5 1.4 5 5 {是} {豆漿} 0.25 0.5 0.5 1.2 5 7 {女} {菜包} 0.25 0.5 0.5 1.1 5 在k=3 項目集中，請找出所有支持度高於0.2之規則。 rule[size(lhs(rule))+size(rhs(rule))==3] |&gt; print.rule.table(all=T, sortBy = &quot;support&quot;) |&gt; column_spec(4, color = &#39;#212F3C&#39;, background = &#39;#FEF9E7&#39;) LHS RHS support confidence coverage lift count 9 {男,是} {燒餅} 0.2 0.8 0.25 3.2 4 10 {男,是} {油條} 0.2 0.8 0.25 2.3 4 11 {男,是} {豆漿} 0.2 0.8 0.25 2.0 4 假設支持度門檻為0.2，信賴度門檻為0.5，請論述「菜包 \\(\\to\\) 柳橙汁」之規則是否成立？ 承題(4)，請論述「燒餅」 \\(\\to\\) 「油條」、「豆漿」之規則是否成立？ print.rule.table(rule, all=T, sortBy = &quot;support&quot;) LHS RHS support confidence coverage lift count 6 {男} {豆漿} 0.30 0.6 0.50 1.5 6 8 {是} {菜包} 0.30 0.6 0.50 1.3 6 1 {男} {燒餅} 0.25 0.5 0.50 2.0 5 2 {否} {奶茶} 0.25 0.5 0.50 1.4 5 3 {是} {油條} 0.25 0.5 0.50 1.4 5 4 {男} {油條} 0.25 0.5 0.50 1.4 5 5 {是} {豆漿} 0.25 0.5 0.50 1.2 5 7 {女} {菜包} 0.25 0.5 0.50 1.1 5 9 {男,是} {燒餅} 0.20 0.8 0.25 3.2 4 10 {男,是} {油條} 0.20 0.8 0.25 2.3 4 11 {男,是} {豆漿} 0.20 0.8 0.25 2.0 4 支持度門檻為0.2，信賴度門檻為0.5下，沒有「菜包 \\(\\to\\) 柳橙汁」和「燒餅 \\(\\to\\) 油條、豆漿」的規則 假設將分析範圍鎖定為男性顧客，請分別論述題(4) ~ (5)之規則是否成立？反之，若鎖定女性顧客，題(4) ~ (5)規則之成立性又為如何？ 假設將分析範圍鎖定為非學生之顧客，請分別論述(4) ~ (5)之規則是否成立？反之，若鎖定學生顧客，題(4) ~ (5)規則之成立性又為如何？ Notice: 設定右手邊只會出現餐點後，餐點好像不會出現在左手邊 調低門檻值 test1 &lt;- apriori(breakfast, parameter = list(support=0.01, confidence=0.01, maxlen=3), appearance = list(default=&quot;lhs&quot;, rhs=unique(target$item))) test1[size(lhs(test1))+size(rhs(test1))==2] |&gt; print.rule.table(all=T, sortBy = &quot;support&quot;) LHS RHS support confidence coverage lift count 51 {男} {豆漿} 0.30 0.6 0.5 1.50 6 54 {是} {菜包} 0.30 0.6 0.5 1.33 6 35 {男} {燒餅} 0.25 0.5 0.5 2.00 5 41 {否} {奶茶} 0.25 0.5 0.5 1.43 5 46 {是} {油條} 0.25 0.5 0.5 1.43 5 47 {男} {油條} 0.25 0.5 0.5 1.43 5 50 {是} {豆漿} 0.25 0.5 0.5 1.25 5 52 {女} {菜包} 0.25 0.5 0.5 1.11 5 30 {女} {柳橙汁} 0.20 0.4 0.5 2.00 4 34 {是} {燒餅} 0.20 0.4 0.5 1.60 4 37 {否} {蛋餅} 0.20 0.4 0.5 1.33 4 43 {男} {奶茶} 0.20 0.4 0.5 1.14 4 55 {男} {菜包} 0.20 0.4 0.5 0.89 4 32 {是} {柳橙汁} 0.15 0.3 0.5 1.50 3 36 {女} {蛋餅} 0.15 0.3 0.5 1.00 3 39 {男} {蛋餅} 0.15 0.3 0.5 1.00 3 40 {女} {奶茶} 0.15 0.3 0.5 0.86 3 49 {否} {豆漿} 0.15 0.3 0.5 0.75 3 53 {否} {菜包} 0.15 0.3 0.5 0.67 3 19 {是} {肉包} 0.10 0.2 0.5 2.00 2 22 {否} {可樂} 0.10 0.2 0.5 2.00 2 24 {否} {漢堡} 0.10 0.2 0.5 2.00 2 25 {男} {漢堡} 0.10 0.2 0.5 2.00 2 26 {女} {吐司} 0.10 0.2 0.5 1.33 2 27 {否} {吐司} 0.10 0.2 0.5 1.33 2 38 {是} {蛋餅} 0.10 0.2 0.5 0.67 2 42 {是} {奶茶} 0.10 0.2 0.5 0.57 2 44 {女} {油條} 0.10 0.2 0.5 0.57 2 45 {否} {油條} 0.10 0.2 0.5 0.57 2 48 {女} {豆漿} 0.10 0.2 0.5 0.50 2 14 {女} {鬆餅} 0.05 0.1 0.5 2.00 1 15 {否} {鬆餅} 0.05 0.1 0.5 2.00 1 16 {否} {煎餃} 0.05 0.1 0.5 2.00 1 17 {男} {煎餃} 0.05 0.1 0.5 2.00 1 18 {女} {肉包} 0.05 0.1 0.5 1.00 1 20 {男} {肉包} 0.05 0.1 0.5 1.00 1 21 {女} {可樂} 0.05 0.1 0.5 1.00 1 23 {男} {可樂} 0.05 0.1 0.5 1.00 1 28 {是} {吐司} 0.05 0.1 0.5 0.67 1 29 {男} {吐司} 0.05 0.1 0.5 0.67 1 31 {否} {柳橙汁} 0.05 0.1 0.5 0.50 1 33 {否} {燒餅} 0.05 0.1 0.5 0.40 1 沒辦法設定兩邊 tryCatch( expr = { test2 &lt;- apriori(breakfast, parameter = list(support=0.01, confidence=0.01, maxlen=3), appearance = list( lhs=unique(c(target$item, target$sex, target$stu)), rhs=unique(target$item))) }, error = function(e) { print(paste(&quot;Error：&quot;, e$message)) } ) ## [1] &quot;Error： The following items cannot be specified in multiple appearance locations: 燒餅, 菜包, 油條, 豆漿, 肉包, 奶茶, 蛋餅, 吐司, 漢堡, 可樂, 煎餃, 柳橙汁, 鬆餅&quot; 改設定左手邊，發現沒有規則，可能是因為沒有東西能放右手邊了 test3 &lt;- apriori(breakfast, parameter = list(support=0.01, confidence=0.01, maxlen=3), appearance = list( lhs=unique(c(target$item, target$sex, target$stu)), default=&quot;rhs&quot;)) test3 ## set of 0 rules 4.6 Groceries data library(arules) library(arulesViz) 4.6.1 ??? \\(\\rightarrow\\) item What are customers likely to buy before buying whole milk? data(&quot;Groceries&quot;) rules&lt;-apriori(data=Groceries, parameter=list(supp=0.001,conf = 0.08), appearance = list(default=&quot;lhs&quot;,rhs=&quot;whole milk&quot;), control = list(verbose=F)) rules&lt;-sort(rules, decreasing=TRUE,by=&quot;confidence&quot;) DATAFRAME(rules[1:5], separate=F) |&gt; format(digits=2) |&gt; knitr::kable() rule support confidence coverage lift count 196 {rice,sugar} =&gt; {whole milk} 0.0012 1 0.0012 3.9 12 323 {canned fish,hygiene articles} =&gt; {whole milk} 0.0011 1 0.0011 3.9 11 1643 {root vegetables,butter,rice} =&gt; {whole milk} 0.0010 1 0.0010 3.9 10 1705 {root vegetables,whipped/sour cream,flour} =&gt; {whole milk} 0.0017 1 0.0017 3.9 17 1716 {butter,soft cheese,domestic eggs} =&gt; {whole milk} 0.0010 1 0.0010 3.9 10 4.6.2 item \\(\\rightarrow\\) ??? What are customers likely to buy if they purchase whole milk? rules&lt;-apriori(data=Groceries, parameter=list(supp=0.001,conf = 0.15,minlen=2), appearance = list(default=&quot;rhs&quot;,lhs=&quot;whole milk&quot;), control = list(verbose=F)) rules&lt;-sort(rules, decreasing=TRUE,by=&quot;confidence&quot;) DATAFRAME(rules[1:5], separate=F) |&gt; format(digits=2) |&gt; knitr::kable() rule support confidence coverage lift count 6 {whole milk} =&gt; {other vegetables} 0.075 0.29 0.26 1.5 736 5 {whole milk} =&gt; {rolls/buns} 0.057 0.22 0.26 1.2 557 4 {whole milk} =&gt; {yogurt} 0.056 0.22 0.26 1.6 551 2 {whole milk} =&gt; {root vegetables} 0.049 0.19 0.26 1.8 481 1 {whole milk} =&gt; {tropical fruit} 0.042 0.17 0.26 1.6 416 "],["collaborative-filtering.html", "Sec 5 Collaborative filtering 5.1 Methods for “realRatingMatrix” Class 5.2 Compare the storage sizes 5.3 Computie Similarity Matrix 5.4 Recommender Model 5.5 Exploratory Data Analysis 5.6 [Rating] Example: Movie 5.7 [Binary] Example: Movie", " Sec 5 Collaborative filtering Collaborative filtering (協同過濾) 會考慮到不同使用者的資訊，其演算法基於“使用者 (user)”和”物件 (item)“的相似性，用於計算的資料為矩陣形式，其中列 (row) 為 user、行 (column) 為 item，對應的值為 rating。 這個矩陣通常會是稀疏 (sparse) 的，尤其是物件很多的情況下，大部分使用者只會買其中幾項，因此這個矩陣會非常龐大且稀疏。 協同過濾的概念很像對遺失值進行插補，因我我們的目的是推薦物品給還未對該物品進行評分的使用者。以電影來說，我們希望推薦使用者它還未看過但可能會感興趣的電影。 訓練及測試 通常在建模的時候會把資料分為訓練集和測試集來檢查模型是否有問題，但在協同過濾中這是比較沒有必要的。 因為前面提到過其概念很像補遺失值，那麼就不會有只補一部份資料的情況。 另外對協同過濾來說沒有實際的”新使用者”，因為新使用者不會有評分的資訊，就沒辦法被推薦。如果已經有評分的資訊，那不如也納進訓練裡來增加樣本，所以沒有必要再拆分資料。 預測的結果會是使用者接受推薦與否，也就是成功或失敗，因此我們能得到一個混淆矩陣: True 1 (Positive) 0 (Negative) Pred. 1 TP FP 0 FN TN 我們會比較專注在“FP”的部分，因為預測它是1表示它和1的那群很相似，所以要推薦或是建議給它。混淆矩陣可以計算很多指標，因為我們會比較在意成功，所以召回率比較重要，舉例來說在路上發傳單，會專注在收了傳單且有來的人。 相似性(similarity) pearson (皮爾森相關係數): \\(m_{pearson(x,y)}=\\frac1n\\sum(\\frac{X-\\bar X}{s_X})(\\frac{Y-\\bar Y}{s_Y})\\) cosine: \\(m_{cosine}=\\frac{X\\cdot Y}{||X||\\cdot||Y||}\\) jacard (二元資料適合使用): \\(J(X,Y)=\\frac{|A\\cap B|}{|A\\cup B|}=\\frac{|A\\cap B|}{|A|+|B|-|A\\cap B|}\\) 演算法 Item-based collaborative filtering (IBCF): 識別同個使用者購買過的物品，推薦相似的物品 根據相似矩陣推薦項目，一旦建立了模型，就不需要訪問原始資料 對於每個項目，模型存儲了 k 個最相似的項目，因此一旦建立了模型，信息量就很小，這在在大量資料下是一個優勢 User-based collaborative filtering (UBCF): 識別相似的使用者，推薦相似使用者購買過的最高評分的物品 需要訪問所有的資料來進行預測，因此大資料不太合適 UBCF的正確率比IBCF稍微高些，因此如果資料不是太大，UBCF是個不錯的選擇 限制 在處理新的使用者或是物品時，這個演算法會有以下的問題，以電影評分為例: 如果新的使用者沒有看過任何電影，IBCF和UBCF都沒辦法推薦，因為兩種演算法的預測都需要基於使用者已經評過分的資訊 如果新的電影沒有被任何人觀看，那它永遠不會被推薦。 該演算法僅使用評分矩陣，但有可能有其他的資訊可以改善推薦，因此有了其他新方法 延伸: Content-based filtering 該演算法以對物品的描述為開始且不需要考慮到其他使用者，而是推薦和使用者過去購買的物品相似的物品 步驟: 定義物品的描述 基於購買紀錄定義使用者檔案 推薦和使用者檔案相符的物品 延伸: Hybrid recommender systems 在很多情況下，我們會建立很多不同的模型，在機器學習中結合不同模型的結果通常會有較好的結果。 可以是平行運算(每個模型分開跑，在綜合結果)或是序列運算(前一個模型的結果是後一個模型的輸入)。 一個簡單的例子是協同過濾結合有關用戶或物品的信息。在IBCF的情況下，物品之間的距離可以同時考慮用戶的偏好和物品的描述。即使在UBCF中，用戶之間的距離也可以考慮他們的偏好和個人數據。 Preview realRatingMatrix類型可用的方法、Pander呈現矩陣 realRatingMatrix和Matrix的儲存空間差異 計算相似矩陣，分為user之間和item之間 recommenderRegistry$get_entries(dataType = \"realRatingMatrix\"): 顯示可用的模型 EDA (Exploratory Data Analysis) Example: Movie 6.1: IBCF 6.2: UBCF Note: recommenderlab::binarize中的minRating參數設定的是被判斷是否為1的最低評分，而不是item被評分的次數。 假設minRating=3，則3(含)以上為1，其它為0 (範例: Binarization) 5.1 Methods for “realRatingMatrix” Class 列出所有 S3 和 S4 generic function 可用的方法 methods(generic.function, class) temp=read.csv(&quot;data/MovieLense.csv&quot;) library(recommenderlab) MovieLense=as(temp,&quot;realRatingMatrix&quot;) DF=getData.frame(MovieLense) methods_matrix &lt;- methods(class = class(MovieLense)) methods_matrix ## [1] [ [&lt;- binarize ## [4] calcPredictionAccuracy coerce colCounts ## [7] colMeans colSds colSums ## [10] denormalize dim dimnames ## [13] dimnames&lt;- dissimilarity evaluationScheme ## [16] getData.frame getList getNormalize ## [19] getRatingMatrix getRatings getTopNLists ## [22] hasRating image normalize ## [25] nratings Recommender removeKnownRatings ## [28] rowCounts rowMeans rowSds ## [31] rowSums sample show ## [34] similarity ## see &#39;?methods&#39; for accessing help and source code 使用pander，以矩陣呈現 使用函數pander(x = NULL, ...) methods_to_print &lt;- as.character(methods_matrix) methods_to_print &lt;- methods_to_print[!grepl(&quot;coerce&quot;, methods_to_print)] methods_to_print &lt;- gsub(&quot;,.*&quot;, &quot;&quot;, methods_to_print, perl = TRUE) methods_to_print &lt;- c(methods_to_print, &quot;&quot;, &quot;&quot;) # 湊齊才能用矩陣 library(pander) pander::pander(matrix(methods_to_print, ncol = 3)) [ dimnames&lt;- nratings [&lt;- dissimilarity Recommender binarize evaluationScheme removeKnownRatings calcPredictionAccuracy getData.frame rowCounts calcPredictionAccuracy getList rowMeans colCounts getNormalize rowSds colMeans getRatingMatrix rowSums colSds getRatings sample colSums getTopNLists show denormalize hasRating similarity dim image dimnames normalize 使用rcode chunk ```{{r render=pander, results=&#39;asis&#39;}} matrix(methods_to_print, ncol = 3) ``` [ dimnames&lt;- nratings [&lt;- dissimilarity Recommender binarize evaluationScheme removeKnownRatings calcPredictionAccuracy getData.frame rowCounts calcPredictionAccuracy getList rowMeans colCounts getNormalize rowSds colMeans getRatingMatrix rowSums colSds getRatings sample colSums getTopNLists show denormalize hasRating similarity dim image dimnames normalize 5.2 Compare the storage sizes 回傳物件分配的空間 object.size(x) Class object size (bytes) data.frame 1725248 realRatingMatrix 1409432 matrix, array 12761360 可以看到recommenderlab的矩陣更簡潔，兩者差了9.05倍 5.3 Computie Similarity Matrix {recommenderlab} 計算評分的相似性 similarity(x, y = NULL, method = NULL, args = NULL, which = &quot;users&quot;, min_matching = 0, min_predictive = 0) method: “cosine”, “pearson”, “jaccard”, etc. which: 計算”users”(rows)或”items”(columns)之間的相似性 min_matching: 最小評分數的閾值 使用者相似性 similarity_users &lt;- similarity(MovieLense[1:5, ], method = &quot;cosine&quot;, which = &quot;users&quot;) class(similarity_users) ## [1] &quot;simil&quot; &quot;dist&quot; similarity_users_as_matrix=as.matrix(similarity_users) similarity_users_as_matrix[nrow(similarity_users_as_matrix):1,] |&gt; pander::pander(plain.ascii = TRUE) 1 2 3 4 5 5 0.9663 0.9924 1 0.9973 NA 4 0.9596 0.9685 0.9565 NA 0.9973 3 0.917 0.9634 NA 0.9565 1 2 0.9803 NA 0.9634 0.9685 0.9924 1 NA 0.9803 0.917 0.9596 0.9663 image(similarity_users_as_matrix, main = &quot;User similarity&quot;) 物品相似性 similarity_items &lt;- similarity(MovieLense[, 1:5], method = &quot;cosine&quot;, which = &quot;items&quot;) similarity_items_as_matrix=as.matrix(similarity_items) similarity_items_as_matrix[nrow(similarity_items_as_matrix):1,] |&gt; pander::pander(digits = 2, plain.ascii = TRUE, split.cells=12) ’Til There Was You (1997) 1-900 (1994) 101 Dalmatians (1996) 12 Angry Men (1957) 187 (1997) 187 (1997) 0.98 1 0.94 1 NA 12 Angry Men (1957) 0.99 1 0.95 NA 1 101 Dalmatians (1996) 0.8 1 NA 0.95 0.94 1-900 (1994) NA NA 1 1 1 ’Til There Was You (1997) NA NA 0.8 0.99 0.98 image(similarity_items_as_matrix, main = &quot;Item similarity&quot;) add.text &lt;- function(simil){ val &lt;- round(as.vector(simil),2) tmp &lt;- data.frame( val=c(val, rep(1,5)), x=c(seq(.25,1,by=.25),seq(.25,1,by=.25)[-1],seq(.25,1,by=.25)[-(1:2)],seq(.25,1,by=.25)[-(1:3)], seq(0,1,length.out=5)), y=c(rep(0,4),rep(.25,3),rep(.5,2),rep(.75,1), seq(0,1,length.out=5)), col=c(ifelse(val&gt;0.97, &quot;white&quot;, &quot;black&quot;), rep(1,5)) ) text(tmp$x, tmp$y, tmp$val, col=tmp$col) } par(mfrow=c(1,2), mar=c(2,2,1,1)) image(similarity_users_as_matrix, main = &quot;User similarity&quot;) add.text(similarity_users) image(similarity_items_as_matrix, main = &quot;Item similarity&quot;) add.text(similarity_items) 5.4 Recommender Model recommender_models &lt;- recommenderRegistry$get_entries(dataType = &quot;realRatingMatrix&quot;) names(recommender_models) ## [1] &quot;HYBRID_realRatingMatrix&quot; &quot;ALS_realRatingMatrix&quot; ## [3] &quot;ALS_implicit_realRatingMatrix&quot; &quot;IBCF_realRatingMatrix&quot; ## [5] &quot;LIBMF_realRatingMatrix&quot; &quot;POPULAR_realRatingMatrix&quot; ## [7] &quot;RANDOM_realRatingMatrix&quot; &quot;RERECOMMEND_realRatingMatrix&quot; ## [9] &quot;SVD_realRatingMatrix&quot; &quot;SVDF_realRatingMatrix&quot; ## [11] &quot;UBCF_realRatingMatrix&quot; lapply(recommender_models, &quot;[[&quot;, &quot;description&quot;) ## $HYBRID_realRatingMatrix ## [1] &quot;Hybrid recommender that aggegates several recommendation strategies using weighted averages.&quot; ## ## $ALS_realRatingMatrix ## [1] &quot;Recommender for explicit ratings based on latent factors, calculated by alternating least squares algorithm.&quot; ## ## $ALS_implicit_realRatingMatrix ## [1] &quot;Recommender for implicit data based on latent factors, calculated by alternating least squares algorithm.&quot; ## ## $IBCF_realRatingMatrix ## [1] &quot;Recommender based on item-based collaborative filtering.&quot; ## ## $LIBMF_realRatingMatrix ## [1] &quot;Matrix factorization with LIBMF via package recosystem (https://cran.r-project.org/web/packages/recosystem/vignettes/introduction.html).&quot; ## ## $POPULAR_realRatingMatrix ## [1] &quot;Recommender based on item popularity.&quot; ## ## $RANDOM_realRatingMatrix ## [1] &quot;Produce random recommendations (real ratings).&quot; ## ## $RERECOMMEND_realRatingMatrix ## [1] &quot;Re-recommends highly rated items (real ratings).&quot; ## ## $SVD_realRatingMatrix ## [1] &quot;Recommender based on SVD approximation with column-mean imputation.&quot; ## ## $SVDF_realRatingMatrix ## [1] &quot;Recommender based on Funk SVD with gradient descend (https://sifter.org/~simon/journal/20061211.html).&quot; ## ## $UBCF_realRatingMatrix ## [1] &quot;Recommender based on user-based collaborative filtering.&quot; extraction function &quot;[[&quot; test &lt;- list(a=1:10,b=letters[1:10]) test ## $a ## [1] 1 2 3 4 5 6 7 8 9 10 ## ## $b ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; &quot;e&quot; &quot;f&quot; &quot;g&quot; &quot;h&quot; &quot;i&quot; &quot;j&quot; &quot;[[&quot;(test,1) ## [1] 1 2 3 4 5 6 7 8 9 10 &quot;[&quot;(test,1) ## $a ## [1] 1 2 3 4 5 6 7 8 9 10 我們只使用IBCF(item-based collaborative filtering)和UBCF(user-based collaborative filtering) 查看參數 df_parameters &lt;- data.frame( parameter = names(recommender_models$IBCF_realRatingMatrix$parameters), default = unlist(recommender_models$IBCF_realRatingMatrix$parameters), row.names = NULL ) df_parameters |&gt; knitr::kable() parameter default k 30 method cosine normalize center normalize_sim_matrix FALSE alpha 0.5 na_as_zero FALSE 5.5 Exploratory Data Analysis dim(MovieLense) ## [1] 943 1664 MovieLense包含943個users和1664部電影 realRatingMatrix屬於S4類別(class)，要用slotNames取得內容名稱 slotNames(MovieLense) ## [1] &quot;data&quot; &quot;normalize&quot; MovieLense@data屬於dgCMatrix類別，屬於稀疏數值矩陣(sparse numeric matrices) class(MovieLense@data) ## [1] &quot;dgCMatrix&quot; ## attr(,&quot;package&quot;) ## [1] &quot;Matrix&quot; dim(MovieLense@data) ## [1] 943 1664 評分為整數，範圍為0~5 vector_ratings &lt;- as.vector(MovieLense@data) unique(vector_ratings) |&gt; sort() ## [1] 0 1 2 3 4 5 table_ratings &lt;- table(vector_ratings) df_ratings &lt;- data.frame( rating = names(table_ratings), occurrences = as.vector(table_ratings) ) df_ratings |&gt; knitr::kable() rating occurrences 0 1469760 1 6059 2 11307 3 27002 4 33947 5 21077 評分為0表示遺失值，因此移除它 length(vector_ratings) vector_ratings &lt;- vector_ratings[vector_ratings != 0] length(vector_ratings) ## [1] 1569152 ## [1] 99392 5.5.1 Rating 大多數評分都大於2，最常見的評分為4 library(ggplot2) ggplot(data.frame(vector_ratings), aes(x=factor(vector_ratings)))+ geom_bar()+ labs(title = &quot;Distribution of the ratings&quot;, x = &quot;Ratings&quot;) 5.5.2 Movie views Star Wars有最高的觀看數 views_per_movie &lt;- colCounts(MovieLense) table_views &lt;- data.frame( movie = names(views_per_movie), views = views_per_movie ) table_views &lt;- table_views[order(table_views$views, decreasing = TRUE), ] ggplot(head(table_views), aes(x=movie, y=views))+ geom_bar(stat=&quot;identity&quot;)+ theme(axis.text.x = element_text(angle = 45, hjust = 1)) + labs(title = &quot;Number of views of the top movies&quot;) 5.5.3 Average ratings 大部分電影的平均評分在3左右，其中有些評分為1或5，可能是因為只有少數人評分 average_ratings &lt;- colMeans(MovieLense) ggplot(data.frame(average_ratings), aes(x=average_ratings))+ geom_histogram()+ labs(title = &quot;Distribution of the average movie rating&quot;, x=&quot;Average Ratings&quot;) 因此我們應該保留大部分人都有評分的電影，保留超過100人觀看的電影，共332部 summary(views_per_movie) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1 7 27 60 80 583 average_ratings_relevant &lt;- average_ratings[views_per_movie &gt; 100] length(average_ratings_relevant) ## [1] 332 移除極端值後，大部分電影的平均評分在4左右 ggplot(data.frame(average_ratings_relevant), aes(x=average_ratings_relevant))+ geom_histogram()+ labs(title = &quot;Distribution of the relevant average ratings&quot;, x=&quot;Average Ratings for views_per_movie &gt; 100&quot;) 5.5.4 Rating Matrix 保留大多人評分的電影和評分大多電影的人 min_n_movies &lt;- quantile(rowCounts(MovieLense), 0.99) min_n_users &lt;- quantile(colCounts(MovieLense), 0.99) image(MovieLense[rowCounts(MovieLense) &gt; min_n_movies, colCounts(MovieLense) &gt; min_n_users], main = &quot;Heatmap of the top 1% users and movies&quot;) 5.5.5 Normalize 因為評分為1~5，圖形呈現為單一色階，因此無法明顯看出電影的好壞，我們可以考慮標準化，讓評分的範圍包含負數及正數且平均為0的資料 太少人看的電影可能會有偏誤(bias) 太少評分的使用者可能會有偏誤 保留評分超過50部電影的使用者和被超過100人評分的電影 ratings_movies &lt;- MovieLense[rowCounts(MovieLense) &gt; 50, colCounts(MovieLense) &gt; 100] min_movies &lt;- quantile(rowCounts(ratings_movies), 0.98) min_users &lt;- quantile(colCounts(ratings_movies), 0.98) image(ratings_movies[rowCounts(ratings_movies) &gt; min_movies, colCounts(ratings_movies) &gt; min_users], main = &quot;Heatmap of the top users and movies&quot;) average_ratings_per_user &lt;- rowMeans(ratings_movies) qplot(average_ratings_per_user) + stat_bin(binwidth = 0.1) + ggtitle(&quot;Distribution of the average rating per user&quot;) + xlab(&quot;Average ratings per user&quot;) Normalize normalize(x, method=&quot;center&quot;, row=TRUE) method: “center”(減平均) or “Z-score”(再除標準差) ratings_movies_norm &lt;- normalize(ratings_movies) min_movies &lt;- quantile(rowCounts(ratings_movies), 0.98) min_users &lt;- quantile(colCounts(ratings_movies), 0.98) image(ratings_movies_norm[rowCounts(ratings_movies_norm) &gt; min_movies, colCounts(ratings_movies_norm) &gt; min_users], main = &quot;Heatmap of the top 2% users and movies&quot;) min_movies &lt;- quantile(rowCounts(ratings_movies), 0.97) min_users &lt;- quantile(colCounts(ratings_movies), 0.97) image(ratings_movies_norm[rowCounts(ratings_movies_norm) &gt; min_movies, colCounts(ratings_movies_norm) &gt; min_users], main = &quot;Heatmap of the top 3% users and movies&quot;) 5.5.6 Binarization min_movies_binary &lt;- quantile(rowCounts(ratings_movies), 0.95) min_users_binary &lt;- quantile(colCounts(ratings_movies), 0.95) 評分為1(含)以上為1，其餘為0 ratings_movies_watched &lt;- binarize(ratings_movies, minRating = 1) image(ratings_movies_watched[rowCounts(ratings_movies) &gt; min_movies_binary, colCounts(ratings_movies) &gt; min_users_binary], main = &quot;Heatmap of the top 5% users and movies&quot;) 評分為3(含)以上為1，其餘為0 ratings_movies_good &lt;- binarize(ratings_movies, minRating = 3) image(ratings_movies_good[rowCounts(ratings_movies) &gt; min_movies_binary, colCounts(ratings_movies) &gt; min_users_binary], main = &quot;Heatmap of the top users and movies&quot;) 5.6 [Rating] Example: Movie library(recommenderlab) library(ggplot2) temp=read.csv(&quot;data/MovieLense.csv&quot;) MovieLense=as(temp,&quot;realRatingMatrix&quot;) ratings_movies &lt;- MovieLense[rowCounts(MovieLense) &gt; 50, colCounts(MovieLense) &gt; 100] # ratings_movies_norm &lt;- normalize(ratings_movies) dim(ratings_movies) ## [1] 560 332 列出兩種演算法的參數 Table 5.1: 兩種演算法的參數: IBCF(左) &amp; UBCF(右) parameter default k 30 method cosine normalize center normalize_sim_matrix FALSE alpha 0.5 na_as_zero FALSE parameter default method cosine nn 25 sample FALSE weighted TRUE normalize center min_matching_items 0 min_predictive_items 0 k: k 個最相似的物品 method: 相似函數 nn: 相似使用者的個數 model = Recommender(data, method, parameter=list(k=30)) getModel(model) 使用getModel取得模型的細節 5.6.1 [Rating] Item-based Collaborative Filtering IBCF考慮使用者的購買紀錄並推薦相似的物品，其核心演算法基於下列的步驟: 對於每兩個物品，測量它們在接收到相似用戶評分方面的相似性 對於每個物品，識別 k 個最相似的物品 對於每個用戶，識別與該用戶購買最相似的物品 5.6.1.1 Build model model_I &lt;- Recommender(data = ratings_movies, method = &quot;IBCF&quot;) model_details &lt;- getModel(model_I) names(model_details) |&gt; matrix(ncol=3) |&gt; pander::pander() description method alpha sim normalize na_as_zero k normalize_sim_matrix verbose model_details$description ## [1] &quot;IBCF: Reduced similarity matrix&quot; sim包含相似矩陣，可以看到方陣的大小和項目數相同(原始資料矩陣) class(model_details$sim) dim(model_details$sim) ## [1] &quot;dgCMatrix&quot; ## attr(,&quot;package&quot;) ## [1] &quot;Matrix&quot; ## [1] 332 332 我們可以看到很多地方等於0，因為每列只有k=30個物品，我們可以透過加總每列數值&gt;0的個數來檢查 image(model_details$sim[1:20,1:20], main = &quot;Heatmap of complete similiarity matrix&quot;) model_details$k ## [1] 30 Hand-on problem 1 table(rowSums(model_details$sim &gt; 0)) apply(model_details$sim,1,function(x) sum(x&gt;0)) |&gt; table() ## ## 30 ## 332 ## ## 30 ## 332 注意: 這個矩陣不是對稱的 因為每列只有前30個相似性會被保留，所以實際上每行的非空元素依賴於相應電影被包含在另一部電影的前 k 名中的次數。 從圖5.1可以看到，有一些電影和很多其他的電影相似，表5.2列出了前6部和最多電影相似的電影 col_sums &lt;- colSums(model_details$sim &gt; 0) DF_col_sums=as.data.frame(col_sums) ggplot(DF_col_sums,aes(x=col_sums)) + geom_histogram() Figure 5.1: Distribution of the column count which_max &lt;- order(col_sums, decreasing = TRUE)[1:6] # rownames(model_details$sim)[which_max] Table 5.2: movies with the most elements movie col_sum Batman Forever (1995) 77 Close Shave, A (1995) 77 Jungle2Jungle (1997) 77 Wrong Trousers, The (1993) 77 Mimic (1997) 73 Ghost and the Darkness, The (1996) 72 5.6.1.2 (Predict) Recommend movies to users 對於每個用戶，該算法提取其評分的電影。 對於每部電影，從相似性矩陣開始識別所有相似的項目。 然後，該算法按以下方式對每個相似的項目進行排名： 提取與此項目相關聯的用戶評分，該評分用作權重 提取與該項目相關聯的每個購買的相似性 將每個權重乘以相關的相似性並加總 然後，該算法識別前 n 個推薦項目 pred_I &lt;- predict(object = model_I, newdata = ratings_movies, n = 6, type=c(&quot;topNList&quot;,&quot;ratings&quot;,&quot;ratingMatrix&quot;)[1]) pred_I ## Recommendations as &#39;topNList&#39; with n = 6 for 560 users. pred_I包含推薦 class(pred_I) slotNames(pred_I) ## [1] &quot;topNList&quot; ## attr(,&quot;package&quot;) ## [1] &quot;recommenderlab&quot; ## [1] &quot;items&quot; &quot;ratings&quot; &quot;itemLabels&quot; &quot;n&quot; items: 每個用戶推薦項目的索引列表 itemLabels: 項目的名稱 n: 推薦數量 Hand-on problem 2 以使用者1為例，表5.3列出了推薦給使用者1的6部電影 pred_I@items[[1]] pred_I@itemLabels[pred_I@items[[1]]] Table 5.3: recommendations for the first user index movie 55 Casablanca (1942) 20 Apt Pupil (1998) 188 Maltese Falcon, The (1941) 218 North by Northwest (1959) 189 Manchurian Candidate, The (1962) 260 Schindler’s List (1993) 我們可以建立一個各使用者的推薦矩陣，表5.4列出了推薦給前4個使用者的6部電影 recc_matrix &lt;- sapply(pred_I@items, function(x){ colnames(ratings_movies)[x] }) dim(recc_matrix) ## [1] 6 560 Table 5.4: recommendations for the first four users 0 1 2 3 Casablanca (1942) 12 Angry Men (1957) Bound (1996) Trainspotting (1996) Apt Pupil (1998) African Queen, The (1951) Donnie Brasco (1997) GoodFellas (1990) Maltese Falcon, The (1941) Alien (1979) In the Line of Fire (1993) Killing Fields, The (1984) North by Northwest (1959) Aliens (1986) My Left Foot (1989) Platoon (1986) Manchurian Candidate, The (1962) Annie Hall (1977) Rainmaker, The (1997) Twelve Monkeys (1995) Schindler’s List (1993) Apocalypse Now (1979) Seven Years in Tibet (1997) Contact (1997) 接著我們可以查看最被推薦的電影，圖5.2顯示了各電影被推薦的次數之分布，我們可以看到大部分的電影只被推薦很少次，而少部分的電影被推薦很多次 number_of_items &lt;- factor(table(recc_matrix)) ggplot(data.frame(x=number_of_items))+ geom_bar(aes(x=x))+ labs(x=&quot;推薦次數&quot;) Figure 5.2: IBCF 推薦次數分布圖 接著我們來查看哪幾部電影被推薦最多次，表5.5列出了前4部被推薦最多次的電影 number_of_items_top &lt;- number_of_items |&gt; sort(decreasing = TRUE) |&gt; head(4) knitr::kable(data.frame( names(number_of_items_top), number_of_items_top ), row.names = F, col.names = c(&quot;Movie&quot;, &quot;Recommend times&quot;), caption = &quot;the most popular movies&quot;) Table 5.5: the most popular movies Movie Recommend times 12 Angry Men (1957) 72 As Good As It Gets (1997) 53 Close Shave, A (1995) 53 Apocalypse Now (1979) 48 5.6.2 [Rating] User-based Collaborative Filtering UBCF演算法步驟: 測量使用者之間的相似性 識別最相似的使用者 2.1. Top k: k nearest neighbors (KNN) 2.2. threshold: 考慮相似性高於門檻值的使用者 計算最相似使用者購買過的物品之評分，評分為相似使用者之評分的平均 3.1. 平均 3.2. 加權平均，以相似性為權重 5.6.2.1 Build model model_U &lt;- Recommender(data = ratings_movies, method = &quot;UBCF&quot;) model_details &lt;- getModel(model_U) names(model_details) |&gt; c(&quot;&quot;,&quot;&quot;) |&gt; matrix(ncol=3) |&gt; pander::pander() description sample min_predictive_items data weighted verbose method normalize nn min_matching_items data包含了評分矩陣，原因是UBCF需要訪問所有的資料來做預測 model_details$data ## 560 x 332 rating matrix of class &#39;realRatingMatrix&#39; with 55298 ratings. ## Normalized using center on rows. 5.6.2.2 (Predict) Recommend movies to users pred_U &lt;- predict(object = model_U, newdata = ratings_movies, n = 6) pred_U ## Recommendations as &#39;topNList&#39; with n = 6 for 560 users. 表5.6列出了UBCF演算法推薦給前4個使用者的6部電影，IBCF推薦的見表5.4 recc_matrix &lt;- sapply(pred_U@items, function(x){ colnames(ratings_movies)[x] }) # dim(recc_matrix) Table 5.6: recommendations for the first four users 0 1 2 3 In the Name of the Father (1993) Welcome to the Dollhouse (1995) Crying Game, The (1992) Godfather: Part II, The (1974) Shine (1996) Amistad (1997) Adventures of Priscilla, Queen of the Desert, The (1994) Gone with the Wind (1939) Close Shave, A (1995) Big Night (1996) Close Shave, A (1995) Big Night (1996) Secrets &amp; Lies (1996) Strictly Ballroom (1992) Wrong Trousers, The (1993) Wizard of Oz, The (1939) Apt Pupil (1998) Blade Runner (1982) Sleepers (1996) Lawrence of Arabia (1962) Cool Hand Luke (1967) Cold Comfort Farm (1995) North by Northwest (1959) Raging Bull (1980) 圖5.3顯示了各電影被推薦的次數之分布，和IBCF演算法的結果(圖5.2)相比，UBCF有更長的尾巴，表示有些電影更常被推薦 number_of_items &lt;- factor(table(recc_matrix)) ggplot(data.frame(x=number_of_items))+ geom_bar(aes(x=x))+ labs(x=&quot;推薦次數&quot;) Figure 5.3: UBCF 推薦次數分布圖 表5.7列出了前4部被UBCF推薦最多次的電影，IBCF的結果見表5.5 number_of_items_top &lt;- number_of_items |&gt; sort(decreasing = TRUE) |&gt; head(4) knitr::kable(data.frame( names(number_of_items_top), number_of_items_top ), row.names = F, col.names = c(&quot;Movie&quot;, &quot;Recommend times&quot;), caption = &quot;the most popular movies&quot;) Table 5.7: the most popular movies Movie Recommend times Close Shave, A (1995) 169 Good Will Hunting (1997) 94 Wrong Trousers, The (1993) 73 Secrets &amp; Lies (1996) 68 5.7 [Binary] Example: Movie 前面我們使用了評分來建立模型，但有可能有下列的情況發生: 我們知道已經購買了哪些項目，但不知道它們的評分 對於每個用戶，我們不知道它購買了哪些項目，但我們知道它喜歡哪些項目 在這些情況下，我們可以建立一個矩陣，其值不是評分，而是0/1資料，如果使用者購買或喜歡該物品則記為1，否則為0。 在我們的情況中，從 ratings_movies 開始，我們可以建立一個 ratings_movies_watched 矩陣，如果用戶看了電影為 1，否則為 0。圖5.4顯示每個使用者觀看了幾部電影的分布，紅色虛線為平均，約為100部 ratings_movies_watched &lt;- binarize(ratings_movies, minRating = 1) qplot(rowSums(ratings_movies_watched)) + stat_bin(binwidth = 10) + geom_vline(xintercept = mean(rowSums(ratings_movies_watched)), col = &quot;red&quot;, linetype = &quot;dashed&quot;) Figure 5.4: Distribution of movies by user 5.7.1 [Binary] Item-based Collaborative Filtering 表5.8列出了IBCF在二元資料推薦給前4個使用者的結果，評分資料的結果見表5.4 bin_model_I &lt;- Recommender( data = ratings_movies_watched, method = &quot;IBCF&quot;, parameter = list(method = &quot;Jaccard&quot;) ) # model_details &lt;- getModel(bin_model_I) recc_predicted &lt;- predict(object = bin_model_I, newdata = ratings_movies_watched, n = 6) recc_matrix &lt;- sapply(recc_predicted@items, function(x){ colnames(ratings_movies_watched)[x] }) Table 5.8: recommendations for the first four users 0 1 2 3 E.T. the Extra-Terrestrial (1982) Empire Strikes Back, The (1980) Raiders of the Lost Ark (1981) Pulp Fiction (1994) Speed (1994) Return of the Jedi (1983) Star Wars (1977) Terminator, The (1984) True Lies (1994) Silence of the Lambs, The (1991) Empire Strikes Back, The (1980) Jurassic Park (1993) Batman (1989) Raiders of the Lost Ark (1981) Indiana Jones and the Last Crusade (1989) Terminator 2: Judgment Day (1991) Mission: Impossible (1996) Indiana Jones and the Last Crusade (1989) Back to the Future (1985) Groundhog Day (1993) Schindler’s List (1993) Back to the Future (1985) Pulp Fiction (1994) Braveheart (1995) 5.7.2 [Binary] User-based Collaborative Filtering 表5.9列出了UBCF在二元資料推薦給前4個使用者的結果，評分資料的結果見表5.6 bin_model_U &lt;- Recommender( data = ratings_movies_watched, method = &quot;UBCF&quot;, parameter = list(method = &quot;Jaccard&quot;) ) recc_predicted &lt;- predict(object = bin_model_U, newdata = ratings_movies_watched, n = 6) recc_matrix &lt;- sapply(recc_predicted@items, function(x){ colnames(ratings_movies_watched)[x] }) Table 5.9: recommendations for the first four users 0 1 2 3 Father of the Bride Part II (1995) Boot, Das (1981) Jungle2Jungle (1997) Philadelphia Story, The (1940) Killing Fields, The (1984) North by Northwest (1959) George of the Jungle (1997) Remains of the Day, The (1993) First Wives Club, The (1996) Willy Wonka and the Chocolate Factory (1971) Fly Away Home (1996) Dead Man Walking (1995) My Left Foot (1989) Mother (1996) Beauty and the Beast (1991) Cool Hand Luke (1967) Fly Away Home (1996) Cold Comfort Farm (1995) Gone with the Wind (1939) As Good As It Gets (1997) Primal Fear (1996) Big Night (1996) Stargate (1994) Deer Hunter, The (1978) "],["self-learning-2.html", "Sec 6 Self-learning 6.1 [EDA] MovieLens 6.2 [CF] MovieLens", " Sec 6 Self-learning 6.1 [EDA] MovieLens 6.1.1 Get data Download ml-latest-small.zip from the newly released MovieLens for education. Compile a dataset as the one, MovieLense.csv, used by code. 列出資料夾中的所有.csv檔案，檢視.csv檔的欄位名稱及內容 list.files(&quot;data/ml-latest-small&quot;) ## [1] &quot;links.csv&quot; &quot;movies.csv&quot; &quot;ratings.csv&quot; &quot;README.txt&quot; &quot;tags.csv&quot; file.list &lt;- paste0(&quot;data/ml-latest-small/&quot;, list.files(&quot;data/ml-latest-small&quot;, pattern = &quot;.csv&quot;)) lapply(file.list, function(x) head(read.csv(x),1)) ## [[1]] ## movieId imdbId tmdbId ## 1 1 114709 862 ## ## [[2]] ## movieId title genres ## 1 1 Toy Story (1995) Adventure|Animation|Children|Comedy|Fantasy ## ## [[3]] ## userId movieId rating timestamp ## 1 1 1 4 964982703 ## ## [[4]] ## userId movieId tag timestamp ## 1 2 60756 funny 1445714994 裡面會用到的是movies.csv和ratings.csv，tag.csv裡的tag是使用者定義的，因此不考慮；而movies.csv裡的genres是從有限個數的類別中選出來的。 保留ratings.csv中的 userId、rating，和movies.csv中的 title、genres，使用merger以movieId串聯兩個資料集 rating &lt;- read.csv(&quot;data/ml-latest-small/ratings.csv&quot;) movies &lt;- read.csv(&quot;data/ml-latest-small/movies.csv&quot;) df.merge &lt;- merge(rating[,-4], movies, by=&quot;movieId&quot;) df.merge &lt;- df.merge[order(df.merge$userId),] head(df.merge, 3) |&gt; knitr::kable(row.names = F) movieId userId rating title genres 1 1 4 Toy Story (1995) Adventure|Animation|Children|Comedy|Fantasy 3 1 4 Grumpier Old Men (1995) Comedy|Romance 6 1 4 Heat (1995) Action|Crime|Thriller 總共有9742部電影，所有使用者看過的電影有9724部，有18部電影沒有被任何使用者看過 library(dplyr) id &lt;- unique(movies$movieId)[!(unique(movies$movieId) %in% unique(rating$movieId))] movies %&gt;% filter(movieId %in% id) %&gt;% knitr::kable() movieId title genres 1076 Innocents, The (1961) Drama|Horror|Thriller 2939 Niagara (1953) Drama|Thriller 3338 For All Mankind (1989) Documentary 3456 Color of Paradise, The (Rang-e khoda) (1999) Drama 4194 I Know Where I’m Going! (1945) Drama|Romance|War 5721 Chosen, The (1981) Drama 6668 Road Home, The (Wo de fu qin mu qin) (1999) Drama|Romance 6849 Scrooge (1970) Drama|Fantasy|Musical 7020 Proof (1991) Comedy|Drama|Romance 7792 Parallax View, The (1974) Thriller 8765 This Gun for Hire (1942) Crime|Film-Noir|Thriller 25855 Roaring Twenties, The (1939) Crime|Drama|Thriller 26085 Mutiny on the Bounty (1962) Adventure|Drama|Romance 30892 In the Realms of the Unreal (2004) Animation|Documentary 32160 Twentieth Century (1934) Comedy 32371 Call Northside 777 (1948) Crime|Drama|Film-Noir 34482 Browning Version, The (1951) Drama 85565 Chalet Girl (2011) Comedy|Romance 但是合併完的資料共有9719部電影 (以標題計算)，檢視以後發現 id 個數比 title 多，表示有標題一樣但 id 不一樣的問題 length(unique(df.merge$movieId)) length(unique(df.merge$title)) ## [1] 9724 ## [1] 9719 id 比 title 多了 5 個， 第一個方法使用基本的套件，第二個方法使用 dplyr 套件，兩種方法找到的不太一樣 duplicated會剔除第一個然後保留重複的，所以!duplicated會保留第一個 dplyr以分組後的個數來挑選 # Method 1: base R test &lt;- df.merge[!duplicated(df.merge$movieId),c(&quot;movieId&quot;,&quot;title&quot;)] test[duplicated(test$title),] |&gt; knitr::kable(row.names = F) movieId title 64997 War of the Worlds (2005) 144606 Confessions of a Dangerous Mind (2002) 26958 Emma (1996) 168358 Saturn 3 (1980) 32600 Eros (2004) # Method 2: dplyr library(dplyr) tmp &lt;- df.merge %&gt;% group_by(title) %&gt;% filter(n()&gt;1) %&gt;% select(c(&quot;movieId&quot;,&quot;title&quot;)) tmp %&gt;% group_by(movieId) %&gt;% filter(n()==1) %&gt;% knitr::kable() movieId title 144606 Confessions of a Dangerous Mind (2002) 147002 Eros (2004) 26958 Emma (1996) 168358 Saturn 3 (1980) 32600 Eros (2004) M1: 顯示不同 id 但 title 相同的資料 M2: 沒有”War of the Worlds (2005)“，因為第2步只保留出現過1次的，但它的兩個id都出現超過1次 有兩個”Eros (2004)“，因為兩個id都只出現一次 # df.merge[df.merge$title==&quot;Emma (1996)&quot;,] # df.merge[df.merge$title==&quot;Confessions of a Dangerous Mind (2002)&quot;,] # df.merge[df.merge$title==&quot;Saturn 3 (1980)&quot;,] %&gt;% knitr::kable() df.merge[df.merge$title==&quot;Eros (2004)&quot;,] %&gt;% knitr::kable(row.names = F) movieId userId rating title genres 147002 318 4.0 Eros (2004) Drama|Romance 32600 606 3.5 Eros (2004) Drama tmp &lt;- df.merge[df.merge$title==&quot;War of the Worlds (2005)&quot;,] tmp[order(tmp$movieId),] %&gt;% tail(5) %&gt;% knitr::kable(row.names = F) movieId userId rating title genres 34048 590 2.5 War of the Worlds (2005) Action|Adventure|Sci-Fi|Thriller 34048 608 4.5 War of the Worlds (2005) Action|Adventure|Sci-Fi|Thriller 34048 610 4.0 War of the Worlds (2005) Action|Adventure|Sci-Fi|Thriller 64997 28 3.5 War of the Worlds (2005) Action|Sci-Fi 64997 68 2.5 War of the Worlds (2005) Action|Sci-Fi 可以看到相同片名但不同id是因為類型(genres)不同 資料轉換成矩陣 Notice: 使用as(df, \"realRatingMatrix\")，df的欄位要按照user, item, rating的順序 library(recommenderlab) df &lt;- df.merge %&gt;% select(&quot;userId&quot;, &quot;title&quot;, &quot;rating&quot;) ratMatr &lt;- as(df, &quot;realRatingMatrix&quot;) dim(ratMatr) ## [1] 610 9719 資料包含610個使用者、9719部電影 儲存空間 data.frame: 3233872 bytes realRatingMatrix: 2180896 bytes matrix: 48358832 bytes (是realRatingMatrix的9.05倍) 6.1.2 EDA slotNames(ratMatr) ## [1] &quot;data&quot; &quot;normalize&quot; rat &lt;- as.vector(ratMatr@data) 評分為0~8，包含小數 7跟8怎麼來的(*´･д･)? knitr::kable(list(data.frame(table(rat), row.names = NULL), data.frame(table(df$rating))), caption = &quot;Rating Matrix (左) &amp; Raw data (右)&quot;) Table 6.1: Rating Matrix (左) &amp; Raw data (右) rat Freq 0 5827758 0.5 1370 1 2811 1.5 1791 2 7550 2.5 5549 3 20047 3.5 13132 4 26816 4.5 8552 5 13211 7 2 8 1 Var1 Freq 0.5 1370 1 2811 1.5 1791 2 7551 2.5 5550 3 20047 3.5 13136 4 26818 4.5 8551 5 13211 從資料發現有些使用者對同一部影片評了兩次，另外這些電影都出現在前面提到的同名不同ID的電影裡，使用者評了兩次有可能是評了不同類型但相同片名的電影。 出現7跟8可能是因為在轉換物件類型時，realRatingMatrix把同個使用者評分同一部電影的評分加總了 df %&gt;% group_by(userId, title) %&gt;% filter(n()&gt;1) ## # A tibble: 8 × 3 ## # Groups: userId, title [4] ## userId title rating ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 28 War of the Worlds (2005) 3.5 ## 2 28 War of the Worlds (2005) 3.5 ## 3 68 War of the Worlds (2005) 2 ## 4 68 War of the Worlds (2005) 2.5 ## 5 111 Confessions of a Dangerous Mind (2002) 4 ## 6 111 Confessions of a Dangerous Mind (2002) 4 ## 7 509 Emma (1996) 3.5 ## 8 509 Emma (1996) 3.5 根據文件，評分採用5星評分制，以半星為單位 (0.5 stars - 5.0 stars)，因此移除其他數值 rat &lt;- rat[rat %in% seq(0.5,5,by=0.5)] 大部分的評分為3, 4星 library(ggplot2) ggplot(data.frame(rat), aes(x=factor(rat)))+ geom_bar()+ labs(title = &quot;Distribution of the ratings&quot;, x = &quot;Ratings&quot;) Star Wars有最高的觀看數 views &lt;- colCounts(ratMatr) tab &lt;- data.frame( movie=names(views), views=views ) tab &lt;- tab[order(tab$views, decreasing = TRUE), ] ggplot(head(tab), aes(x=movie, y=views))+ geom_bar(stat=&quot;identity&quot;)+ theme(axis.text.x = element_text(angle = 45, hjust = 1)) + labs(title = &quot;Number of views of the top movies&quot;) 6.1.3 Adjust plot output Please sort the x-axis by views, rather than by alphabet. ggplot(head(tab), aes(x=reorder(movie,views,sum, decreasing=T), y=views))+ geom_bar(stat=&quot;identity&quot;)+ theme(axis.text.x = element_text(angle = 45, hjust = 1)) + labs(title = &quot;Number of views of the top movies&quot;, x=&quot;movie&quot;) 6.1.4 Average Ratings 大部分電影的平均評分在3.5左右，其中有些評分為1或5，可能是因為只有少數人評分 avg_rat &lt;- colMeans(ratMatr) ggplot(data.frame(avg_rat), aes(x=avg_rat))+ geom_histogram()+ labs(title = &quot;Distribution of the average movie rating&quot;, x=&quot;Average Ratings&quot;) 保留超過50和100人觀看的電影 quantile(views, .95); quantile(views, .99) ## 95% ## 47 ## 99% ## 115 avg_rat_95 &lt;- avg_rat[views &gt; 50] avg_rat_99 &lt;- avg_rat[views &gt; 100] 移除少數人觀看的電影後，大部分電影的平均評分在3.8左右 p1 &lt;- ggplot(data.frame(avg_rat_99), aes(x=avg_rat_99))+ geom_histogram()+ labs(title = &quot;Top 1% views average ratings&quot;, x=&quot;Average Ratings for views_per_movie &gt; 100&quot;) p2 &lt;- ggplot(data.frame(avg_rat_95), aes(x=avg_rat_95))+ geom_histogram()+ labs(title = &quot;Top 5% views average ratings&quot;, x=&quot;Average Ratings for views_per_movie &gt; 50&quot;) library(gridExtra) grid.arrange(p1, p2, nrow=1) 6.1.5 Rating Matrix 保留大多人評分的電影和評分大多電影的人 min_movies &lt;- quantile(rowCounts(ratMatr), 0.995) min_users &lt;- quantile(colCounts(ratMatr), 0.995) image(ratMatr[rowCounts(ratMatr) &gt; min_movies, colCounts(ratMatr) &gt; min_users], main = &quot;Heatmap of the top 0.5% users and movies&quot;) 6.1.6 Normalize 保留評分超過400部電影的使用者和被超過100人評分的電影 quantile(rowCounts(ratMatr), .9) ## 90% ## 400 quantile(colCounts(ratMatr), .99) ## 99% ## 115 rat_movies &lt;- ratMatr[rowCounts(ratMatr) &gt; 400, colCounts(ratMatr) &gt; 100] min_movies &lt;- quantile(rowCounts(rat_movies), .9) min_users &lt;- quantile(colCounts(rat_movies), .9) Normalize normalize(x, method=&quot;center&quot;, row=TRUE) method: “center”(減平均) or “Z-score”(再除標準差) rat_movies_norm &lt;- normalize(rat_movies) image(rat_movies_norm[rowCounts(rat_movies_norm) &gt; min_movies, colCounts(rat_movies_norm) &gt; min_users], main = &quot;Heatmap of the top 10% users and movies&quot;) 6.1.7 Binarization 二元分類 binarize(ratingMatrix, minRating = n) minRating: 評分大於等於minRating的為1，其餘為0 評分4(含)以上的為1，其餘為0 rat_movies_watched &lt;- binarize(rat_movies, minRating = 4) image(rat_movies_watched[rowCounts(rat_movies) &gt; min_movies, colCounts(rat_movies) &gt; min_users], main = &quot;Heatmap of the top 10% users and movies&quot;) 6.2 [CF] MovieLens temp=read.csv(&quot;data/MovieLense.csv&quot;) MovieLense=as(temp,&quot;realRatingMatrix&quot;) ratings_movies &lt;- MovieLense[rowCounts(MovieLense) &gt; 50, colCounts(MovieLense) &gt; 100] detail &lt;- Recommender(data = ratings_movies, method = &quot;IBCF&quot;) |&gt; getModel() sim &lt;- detail$sim |&gt; as.matrix() # sim &lt;- ifelse(sim==0, NA, sim) 6.2.1 Use similarity() to recheck Hand-on problem 1 similarity_items &lt;- similarity(ratings_movies, method = &quot;cosine&quot;, which = &quot;items&quot;) X=as.matrix(similarity_items) 避開Sim=1的部分 每列取前30個 class(X) ## [1] &quot;matrix&quot; &quot;array&quot; X[1:3,1:3] |&gt; pander::pander(row.names=F) 101 Dalmatians (1996) 12 Angry Men (1957) 2001: A Space Odyssey (1968) NA 0.9491 0.9378 0.9491 NA 0.9821 0.9378 0.9821 NA 我們可以看到similarity計算出來自己跟自己的相似度為NA，因此不用擔心取前30個最相似的電影會挑到自己 top30 &lt;- apply(X, 1, function(x){ top_val &lt;- sort(x, decreasing = T) |&gt; head(30) out &lt;- x out[!(x %in% top_val)] &lt;- NA return(out) }) |&gt; t() apply(top30,1,function(x) sum(x&gt;0, na.rm = T)) |&gt; table() ## ## 30 ## 332 確認每列都只有保留30個相似度 identical(top30, sim) top30[1:4,1:3] |&gt; pander::pander(row.names=F) sim[1:4,1:3] |&gt; pander::pander(row.names=F) ## [1] FALSE 101 Dalmatians (1996) 12 Angry Men (1957) 2001: A Space Odyssey (1968) NA NA NA NA NA NA NA 0.9821 NA NA NA NA 101 Dalmatians (1996) 12 Angry Men (1957) 2001: A Space Odyssey (1968) 0 0 0 0 0 0 0 0 0 0.6645 0 0 Normalize 但是和使用Recommender得到的相似度不一樣，從表5.1可以看到normalize預設是”center”，因此我們嘗試先把資料的平均歸到0。 normalize(x, method=&quot;center&quot;, row=TRUE) center_ratings_movies &lt;- normalize(ratings_movies) center_X &lt;- similarity(center_ratings_movies, method = &quot;cosine&quot;, which = &quot;items&quot;) |&gt; as.matrix() center_top30 &lt;- apply(center_X, 1, function(x){ top_val &lt;- sort(x, decreasing = T) |&gt; head(30) out &lt;- x out[!(x %in% top_val)] &lt;- NA return(out) }) |&gt; t() identical(center_top30, sim) ## [1] FALSE 可以發現把realRatingMatrix中每列都正規化到平均為0後，再計算相似度就可以得到和Recommender一樣的數值 6.2.2 Similarity: try using robust statistic (skip) 6.2.3 Why the following code get different results between Hand-on problem 2 ratingTest=as(ratings_movies,&quot;matrix&quot;) rating.test &lt;- ifelse(is.na(ratingTest),0,ratingTest) # dim(rating.test) # 560 x 332 # dim(as.matrix(detail$sim)) # 332 x 332 # dim(sim) # 332 x 332 Recom=rating.test %*% sim # dim(Recom) # 560 x 332 表6.2列出了上面程式碼中計算的結果，可以看到和Recommender推薦的結果(表5.3)不太一樣 data.frame( movie = Recom[1,][order(Recom[1,], decreasing = TRUE)][1:6] ) |&gt; knitr::kable(caption = &quot;前6部推薦的電影&quot;) Table 6.2: 前6部推薦的電影 movie Close Shave, A (1995) 155 Wrong Trousers, The (1993) 147 Chasing Amy (1997) 134 Usual Suspects, The (1995) 123 Shawshank Redemption, The (1994) 114 Good Will Hunting (1997) 110 從這一節中可以知道Recommender的預設是會對資料先進行正規化的，所以這邊也同樣先嘗試對ratings_movies正規化後得到center_ratings_movies ratingTest=as(center_ratings_movies,&quot;matrix&quot;) norm_rating.test &lt;- ifelse(is.na(ratingTest),0,ratingTest) new_Recom=norm_rating.test %*% sim 表6.3列出了正規化後的結果，可以看到和表6.2相比只有一個不同 data.frame( movie = new_Recom[1,][order(new_Recom[1,], decreasing = TRUE)][1:6] ) |&gt; knitr::kable(caption = &quot;正規化後前6部推薦的電影&quot;) Table 6.3: 正規化後前6部推薦的電影 movie Wrong Trousers, The (1993) 23 Close Shave, A (1995) 19 Sling Blade (1996) 18 Chasing Amy (1997) 18 Star Wars (1977) 17 Usual Suspects, The (1995) 16 從得到的Recom可以發現它計算了所有的評分，也就是說它把已經看過的電影也一起計算了 is.na(Recom) |&gt; table() ## ## FALSE ## 185920 rat.mat1 &lt;- as(ratings_movies[1,], &quot;matrix&quot;) movie1 &lt;- rat.mat1 |&gt; colnames() rat.mat1[,c(&quot;Wrong Trousers, The (1993)&quot;,&quot;Close Shave, A (1995)&quot;)] ## Wrong Trousers, The (1993) Close Shave, A (1995) ## 5 NA 我們可以看到是”Wrong Trousers, The (1993)“已經被評過分了，所以在查看時要先排除已經被看過的電影 recc1 &lt;- Recom[1,][is.na(rat.mat1)] data.frame( movie = recc1[order(recc1, decreasing = TRUE)][1:6] ) |&gt; knitr::kable(caption = &quot;未正規化&quot;) new_recc1 &lt;- new_Recom[1,][is.na(rat.mat1)] data.frame( movie = new_recc1[order(new_recc1, decreasing = TRUE)][1:6] ) |&gt; knitr::kable(caption = &quot;正規化&quot;) Table 6.4: 未正規化 movie Close Shave, A (1995) 155 Casablanca (1942) 102 Lawrence of Arabia (1962) 101 As Good As It Gets (1997) 97 Manchurian Candidate, The (1962) 95 Secrets &amp; Lies (1996) 93 Table 6.4: 正規化 movie Close Shave, A (1995) 19 Casablanca (1942) 14 Schindler’s List (1993) 13 L.A. Confidential (1997) 12 Lawrence of Arabia (1962) 12 Rear Window (1954) 11 兩種結果只有2部電影不同，和Recommender推薦相同的只有”Casablanca (1942)“和”Schindler’s List (1993)“兩部 根據表5.1可以看到參數中還有一個\\(\\alpha\\)預設為0.5，這裡提到\\(\\alpha\\)被用來排除項目預測因子，以減少熱門項目的偏見，並尋找更具區別性的項目。應該是一個懲罰項的作用 "],["final-project.html", "Sec 7 Final Project", " Sec 7 Final Project slide here "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
